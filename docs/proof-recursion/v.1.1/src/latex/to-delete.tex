% !TEX root = ../build-polygon/proof-recursion.tex

%TODO Add forkId (version of the zkEVM)

\section{Introduction}

This document specifies how the polygon zkEVM is proven using 
recursion, agregation and composition.
The constraints of the zkEVM are specified as polynomial identities using the 
PIL language. Then, an execution trace can be proven using the PIL specification for 
building a STARK that is proved with the FRI protocol.
The problem is that STARKs generate big proofs. 
This document describes how to use recursion together with composition to shorten the prove size.

In a high level, a basic recursion block transforms a PIL specification into the 
specification of its STARK verification circuit (written in Circom). 
The circuit verifies the STARK of the PIL specification. 
Then, the Circom specification is transformed into a Plonkish PIL specification and the process is iterated.

In addition to recursion, also aggregation is implemented so that provers 
can aggregate the proofs of multiple transaction batches. 


\ifBACKGROUND
\section{Background}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SNARK Composition} \label{sec:composition}

Imagine that we have two SNARK proof systems, $\A$ and $\B$, with different cost profiles. For example, assume that the prover in the first $\A$ is very fast, but the proof size is too large and the verification time is so slow to be directly deployed in practise. In contrast, the prover is second $\B$ is really slow, but the proof size is short and the verification time as good as it can be. Even if this looks like a very artificial scenario, this actually happens very often in the real world; where different state-of-the-art SNARKs have different trade-offs and therefore one has to choose the best one, in terms of costs, for his specific use case.

The previous situation depicts the following question: is it possible to combine them in some way to obtain a SNARK that inherits the best costs of both $\A$ and $\B$? More specifically, we want to obtain a SNARK $\C$ with the fast prover of $\A$ and the short proof size and fast verification time of $\B$.

Theoretically, it is possible via \textit{proof composition} which works as follows. Denote by $(\P_X, \V_X)$ the prover and the verifier of some SNARK $X$.  Say that $\P_{\C}$ claims to know a witness for a particular statement. First, $\P_{\C}$ uses $\P_{\A}$ to generate a SNARK proof $\pi$ of the claim. However, since $\pi$ is big and slow to verify, $\P_{\C}$ does not directly send $\pi$ to $\P_{\C}$. Instead, $\P_{\C}$ uses $\B$ to generate a proof $\pi'$ for the following statement:
\begin{center}
	``I know a proof $\pi$ for the original statement that makes $\V_{\A}$ accept with high probability.''
\end{center}
Then, $\P_{\C}$ sends $\pi'$ to $\V_{\C}$. In other words, $\P_{\C}$ uses $\P_{\A}$ to generate a proof $\pi$, then writes down the verification procedure of $\V_{\A}$ as a statement, uses $\P_{\B}$ to generate the proof $\pi'$ and finally uses $\V_{\B}$ to verify $\pi'$. Figure \ref{fig:composition} shows a diagram for this whole process.

It is important to remark here that proof composition requires taking the verification procedure as carried on by $\V_{\A}$ and written down in the underneath computational model used by $\B$. Hence, if arithmetic circuits are used, then one must write the verification part as an arithmetic circuit satisfiability instance.

\begin{figure}[H]
	\centering
	\includegraphics[width=.75\textwidth]{\recursiondir/figures/composition-diagram}
	\caption{Resulting statement obtained from a (depth-$1$) SNARK composition.}
	\label{fig:composition}
\end{figure}

\paragraph*{Costs of the Composition.}
Let's now discuss the costs of $\C$. Since $\P_{\C}$ only sends the second proof $\pi'$ to $\V_{\C}$, the proof size of $\C$ is the size of $\pi'$ as generated by $\V_{\B}$. Moreover, the verification time is precisely the time that $\V_{\B}$ takes to verify $\pi'$. Since the proof size and verification time of $\B$ are short and fast, respectively, the proof size and verification time of $\C$ are short and fast as well.

On the other hand, $\P_{\C}$ first has to generate the proof $\pi$ using $\P_{\A}$, and then has to generate the proof $\pi'$ using $\P_{\B}$. As commented at the beginning of the section, $\P_{\A}$ is fast whereas $\P_{\B}$ is slow. Wouldn't this imply that $\P_{\C}$ is slow? The key point is that the circuit of the verification procedure of $\V_{\A}$ is much more small than the circuit used to compute the first proof, since the verification time taken by $\V_{\A}$ is small (even if not optimal). Hence, the time required by $\P_{\C}$ to generate the second proof $\pi'$ is tiny compared to the time required to compute $\pi$. So, the proving time of $\P_{\C}$ is almost equal to the proving time of $\P_{\A}$, which is fast by assumption.

The best costs of both SNARKs have been achieved.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deeper Composition and Proof Recursion}

In the previous section we have seen how SNARK composition looks like and explained it with the generic case of two distinct SNARKs. In this section we explore what happens when we compose a SNARK with itself many times. This procedure is known as \textit{proof recursion}, and its name originates from the resemblance of any computer program being able to be evaluated at itself.

Say that we have a SNARK $\A$ with both proof size and evaluation time of size $O\left(N^{1/2}\right)$. Composing $\A$ with itself yields a new SNARK with proof size and verification time $O\left(\left(N^{1/2}\right)^{1/2}\right) = O\left(N^{1/4}\right)$. One more invocation of $\A$ will yield a new SNARK with proof size and verification time $O\left(N^{1/8}\right)$. One might be attempted to think that this composition can be carried on as many times as he would like, so that proof size and verification time fits with his objectives. However, there exists some upper bound, known as the \textit{recursion threshold}, to the minimum size that can be reached. This threshold refers to the smallest circuit size $N^{\star}$ such that the verification procedure of the initial SNARK $\A$ cannot be represented by a circuit of size smaller than $N^{\star}$. One example of a recursion threshold is the one obtained by Halo \cite{EPRINT:BowGriHop19}, which was proven to be as big as $2^{17}$. Not surprisingly, if one reaches the threshold and instantiate $\A$ another time, not only the resulting SNARK does not reduce costs, but in fact increase them.

What happens with the proving time? Simply put: the more compositions the resulting SNARK is composed of, the more work the prover has to do. Recall from the previous section that the prover must compute one proof per instantiation of the SNARK. For example, if $\A$ is composed with itself three times, the computer first computes a proof $\pi$ that would convice the $\A$ verifier, then produce a proof $\pi'$ that it knows a valid $\pi$, and finally a proof $\pi''$ that it knows a valid $\pi'$. Ever if the consequent proofs take substantially less time than the first one, there is some more work to do that just producing $\pi$.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Further Applications}

In the previous section we have seen that SNARK composition can be used to improve the overall efficiency of existing SNARKs. Even if complex, this is a generic strategy to improve state-of-the-art SNARKs to whom is unclear if some optimizations are possible, or even when such optimizations are constrained by theoretical bounds. Importantly, applications of proof composition are not restricted to be of efficiency nature. In this section we explore some more applications of proof composition.



\paragraph*{Iterative Computation.} An interesting application of proof recursion is the one of \textit{iterative computation}. In this context, a designated prover $\P$ intends to prove to a verifier $\V$ that for some (public or secret) input $x$ and a publicly known function $f$ we have that:
\[
	f^{(i)}(x) := \underbrace{f(f(\dots f(f}_{i}(x)) \dots)) = y,
\]
where $y$ is a public output. One might notice that applying the same function $f$ some amount of times to an input $x$ is the same as function composition. Hence, one can build SNARKs for iterative computation by generating a SNARK of a single iteration of $f$ on $x$ and then composing the SNARK with itself the requested number of times.

\paragraph*{Incrementally Verifiable Computation.} A generalization of iterative computation is \textit{incrementally verifiable computation}. The idea is that, after each iteration $i$ of $f$ on $x$, $\P$ is able to output a partial computation $y_i$ and a proof $\pi_i$ that $f^{(i)}(x) = y_i$. Then, possibly before performing another iteration, a verifier is able to check the proof $\pi_i$ and be sure with high probability that $f$ has been correctly applied up until this point. Moreover, any party can resume the computation from that point and still be able to generate subsequent proofs.

\paragraph*{Proof Aggregation.} \textit{Proof aggregation} is a particular type of proof composition in which multiple valid proofs can be all proven to be valid by comprising them all into one proof, called the \textit{aggregated proof}, and only validating the aggregated one. More formally, $T$ parties (possibly the same one) $\P_1, \dots, \P_T$ use some proof system to produce proofs $\pi_1, \dots, \pi_T$. Then, instead of having one verification procedure per each proof, composition is used to prove the following statement:
\begin{center}
	I know valid proofs $\pi_1, \dots, \pi_T$ such that $C(\pi_1, \dots, \pi_T) = 0$,
\end{center}
where $C$ is the arithmetic circuit representing the verification procedure of all $\pi_1, \dots, \pi_T$.

The notion of proof aggregation becomes useful in the context of distributed computation, where a computation is too heavy to be carried out by a single party. Instead, the party would break the computation in $T$ pieces and send each of these pieces to a different party, who would produce a proof along with the output of each part. Finally, the original party would combine all the pieces into the actual output and produce a proof of the whole computation.


\paragraph*{Application to Blockchain Scalability.} Some mentioned applications could be combined to achieve \textit{scalability of blockchains}. That is, in order for a blockchain to improve its throughput (i.e., number of transactions per second), which is typically limited by block size and block finding speed, one can choose to perform transactions off-chain and publish a proof of correct state evolution on-chain.

There are several ways of accomplishing the off-chain proof. One can go for computing a proof of a blockchain's state update due to transactions within a block and then simply verify the proof on-chain. Even if totally possible, this is not the lowest cost solution since a blockchain's scalability solution must consume as less storage and verification time as possible. Because of these objectives, a more appropriate proof management design can be obtained using proof composition and/or proof aggregation.
Figure \ref{fig:composition-blockchain} provides a possible proof-management design through a combination of both proof techniques.
\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{\recursiondir/figures/composition-blockchain}
	\caption{Design candidate's diagram of a proving scheme for the blockchain scalability issue.}
	\label{fig:composition-blockchain}
\end{figure}
As it can be observed, there is a trade-off between the number of blocks to aggregate and time required to perform such proofs. Finding the best scheme for off-chain proving is still an active area of research since there are many variables that come into the place. Which SNARKs to choose, how to combine them efficiently or how to organize the proof generalization are still open questions for those technologies building on top of thes ideas.

The prominent realization of this idea are \textit{rollups} \cite{Thibault2022}, \cite{Lavaur2022}. Section \ref{sec:scalability} will develop on the details about the core ideas of rollups.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{What about Zero-Knowledge?}

There is a good reason why zero-knowledge has not been discuseed up until this point in the whole section. The key fact is that the resulting composed SNARK satisfies the zero-knowledge property if and only if the last SNARK in the composition satisfies it. More exemplified, if $\B$ satisfies zero-knowledge, then so the composition of any other (and any number) SNARK with $\B$ as long as $\B$ is computed the last.

Hence, here we encounter a new potential benefit of proof composition: composing a highly efficient but not zero-knowledge SNARK $\A$ with a zero-knowledge SNARK $\B$ yields a zero-knowledge (and probably highly efficient) SNARK $\C = \B \circ \A$. 

\fi





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TODO:¿¿ \subsubsection{Modular Design}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tools}

\subsection{PIL}

Polynomial Identity Language (PIL) is a novel domain-specific language to define the constraints of computation traces of either computations based on the circuit model or computations based on the state machine model. 
The constraints of the zkEVM execution trace, which is based on a state machine, are specified with PIL.


\subsection{Circom}

In Figure \ref{fig:circom_architecture} we show the architecture of Circom.
Programmers can use the Circom language to define arithmetic circuits and the compiler generates a file with the set of associated R1CS constraints together with a program (written either in cpp or wasm) that can be run to efficiently compute a valid assignment to all wires of the circuit.

After compiling a circuit, we can calculate all the signals that match the set of constraints of the circuit using the cpp or wasm programs generated by the compiler. To do so, we simply need to provide a file with a set of valid input values, and the program will calculate a set of values for the rest of signals of the circuit. A valid set of input, intermediate and output values is called \textit{witness}.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{\recursiondir/figures/circom-architecture}
\caption{Circom}
\label{fig:circom_architecture}
\end{figure}



\subsection{Non-recursive STARK \label{subsec:non_recursive_STARK}}

The setting for STARK proofs are machine-like computations from which we can derive a certain arithmetization, 
giving us a set of constraints describing its correct execution. 
The polynomial building the constraints that arise from a certain arithmetization can, 
in fact, depend on the inputs of the state machine itself giving what we call \textit{committed polynomials} and, 
by definition, should be computed once per proof. 
However, a polynomial that is completely independent of the input values so it is kept constant among several executions of the same state machine should be computed only once per arithmetization. 
The former kind of polynomials are called \textit{constant polynomials} and represent the computation that is being executed, so that they are publicly available for both the prover and the verifier, unlike the committed ones, which are not directly available to the verifier.


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\recursiondir/figures/pil-stark-no_recursive-blocks}
\caption{Non recursive STARK.}
\label{fig:non-recursive}
\end{figure}

Due to the fact that all the constant polynomials do not depend on the particular inputs of a certain computation executed by the state machine, we can split the processes that make up the generation of a proof in two: the \textit{setup phase} and the \textit{proving phase}. 
This allows to execute both processes separately, which is highly important in order not to execute the constants' computation per proof, drastically increasing the proving time. 

The \textit{setup phase} performs all the pre-processing. 
The setup is only done once per state machine definition, allowing to reuse it whether the definition does not change. 
The definition of a state machine with \texttt{pil-stark} has three parts:

\begin{compactitem}
\item The \texttt{rom.json} file to build the computation.
\item The configuration of the FRI used in the STARK proof, inside a \texttt{starkstruct} file, specifying the blowup factor, the size of the trace and its LDE (Low Degree Extension) and the number of queries to be performed.
\item The \texttt{PIL} description of the STARK state machine, that is to say, the constraints that define the correctness of the execution trace (\texttt{file.pil} file).
\end{compactitem}

The PIL file and the \texttt{starkstruct} are used by the \texttt{build STARK info} process to write a file called \texttt{starkInfo} containing, apart from all the FRI-related parameters, several useful fields related with the PIL and the shape of the constraints. The PIL description is parsed with a compiler called \texttt{pilcom}\footnote{The \texttt{pilcom} parser is written with \url{https://github.com/zaach/jison}, which generates bottom-up parsers in JavaScript.} to obtain a parsed JSON version of the PIL, which will be used by the prover to compute all the polynomials involved in the proving procedure. The \texttt{build constants} process, using the parsed PIL, computes the evaluations of the constant polynomials over the evaluation domain determined by the \texttt{starkstruct}. Additionally, once this evaluations are computed, the \texttt{build constant tree} process generates the Merkle root of its corresponding tree for the verifier, which we call constant root (\texttt{constRoot}). 

In the \textit{proving phase}, the Prover executes all the processes that, given an input, generate a proof for the computation. 
The STARK executor process computes the evaluations of the polynomials that are going to be committed. To do so, it takes the names and descriptions of the polynomials from the parsed PIL and the provided inputs. Observe that, since the values of the committed polynomials are strongly dependent on the inputs, this procedure should be executed once per proof, unlike the setup phase. 

Finally, the \texttt{pil-stark} STARK prover process takes the evaluations of both the constant and the committed polynomials of the previous steps and all the information stored in the \texttt{starkInfo} object in order to generate the corresponding STARK proof and the associated public values for which the proof is valid. 
We use the \textsf{eSTARK} protocol, which is specially designed to proof PIL statements. 
The \textsf{eSTARK} protocol is composed on two main stages:

\begin{itemize}
\item \textit{Low-Degree Reduction phase}: Following \cite{EPRINT:StarkWare21}, first of all, we obtain a polynomial called FRI which codifies the validity of the values of the trace according to the PIL into the fact that it has low degree. This polynomial is committed to the verifier, as well as several previous polynomials that are used to provide consistency checks between them.  This phase, however, differs from the one described in \cite{EPRINT:StarkWare21} because PIL also accepts, apart from polynomial equalities, arguments such lookups, permutations or even copy-constraints (called connection arguments). Hence, this phase needs to be adapted in order to proof the correctness of each of the enumerated arguments. This serves as a motivation to call this protocol \textsf{eSTARK}, standing for \textit{extended STARK}. 

\item \textit{FRI phase}: After obtaining the so called FRI polynomial, the prover and the verifier are involved into a FRI Protocol \cite{ICALP:BBHR18}, aiming for proving that the committed polynomial has low degree (more concretely, it proves that the committed values of the polynomials raise a function that is close enough to a polynomial of low degree, see \cite{ICALP:BBHR18} for more information on FRI Protocol). 
\end{itemize}

The first stage can, in turn, be divided into several rounds. Below, we describe in a high level what is each of the rounds aiming.

\begin{itemize}

\item \textit{Round 1:} Given the trace column polynomials interpolating the execution trace, the prover commits to them.

\item \textit{Round 2:} The prover commits, for each lookup argument, to the $h$-polynomials of the modified \plookup version described in \cite{EPRINT:PFMBM22} (see \cite{EPRINT:GabWil20} for more information about \plookup protocol).

\item \textit{Round 3:} The prover commits to the grand-product polynomials for each of the arguments appearing in the PIL together with some intermediate polynomials used to reduce the degree of the grand products. This is due to the fact that \texttt{pil-stark} imposes a degree bound when committing to a polynomial. See \cite{EPRINT:GabWilCio19, EPRINT:GabWil20} for the specification of the grand-products of each of the different arguments allowed in PIL.

\item \textit{Round 4:} The prover commits to the $2$ polynomials $Q_1, Q_2$ arising from the splitting of the quotient polynomial $Q$. 

\item \textit{Round 5:} The prover provides the verifier with all the necessary evaluations of the polynomials so that he/she can execute the corresponding checks. 

\item \textit{Round 6:} The prover receives two randomness from the verifier which are used to construct the previously described FRI polynomial. Then, the prover and the verifier are involved into a FRI Protocol, ending with the prover sending the corresponding FRI proof to the verifier. 

\end{itemize} 


After the proof is generated, it is sent to the Verifier instance so that he/she can start the verification procedure, after what will accept or reject the proof. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composition, Recursion and Aggregation}

\subsection{Composition}

As shown in Section \ref{subsec:non_recursive_STARK}, the basic verification of a STARK is performed by a verifier entity using the proof, the publics and some other verifier parameters.
Composing proofs means using different proving systems together to generate a proof.
Generally composition is used to increase the efficiency of some part of the system.
In our case, as first proving system we use a STARK and our main idea of composition is to delegate the verification procedure of the STARK proof $\pi_{STARK}$ to a verification circuit $C$.
In this case, if the prover provides a proof for the correct execution of the verification circuit $\pi_{CIRCUIT}$, then this is enough to verify the original STARK.
As shown in Figure \ref{fig:simple-composition}, in this case, 
the verifier entity just verifies the proof of the STARK verification circuit $\pi_{CIRCUIT}$.
The advantage of this composition is that $\pi_{CIRCUIT}$ is smaller and faster to verify than $\pi_{STARK}$.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{\recursiondir/figures/simple-composition}
\caption{Simple composition.}
\label{fig:simple-composition}
\end{figure}

\subsection{Recursion}

\subsubsection{Setup Phase}

Since verifiers are much more efficient than provers, we can use this fact to create a recursive cascade of verifiers in which at each step we achieve a proof that can be more efficiently verified.
In our architecture, we create a chain of STARK verifiers using intermediate circuits for the definition of these STARK verifiers as shown in Figure \ref{fig:recursion-setup}.
We use circuits because they are suitable for computations with limited branching and a verifier is a computation of this type.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{\recursiondir/figures/recursion-setup}
\caption{Recursion setup.}
\label{fig:recursion-setup}
\end{figure}

Following Figure \ref{fig:recursion-setup}, lets consider that 
we have the parameters that describe the first STARK (pil, constants and starkinfo).
This first STARK that we denote as $\texttt{STARK}_{\texttt{A}}$, is then automatically translated into its verifier circuit. 
The STARK verifier circuit is described with R1CS constraints.
This translation, that we call \stoc (STARK-to-CIRCUIT), is performed in a setup phase. In other words, the R1CS description of the STARK verifier circuit can be pre-processed before the computation of the proof. 
We use Circom as intermediate representation language for the description of the circuits (see Section \ref{subsec:stoc} for more details about \stoc).

Next, we take the circuit definition (R1CS) and automatically translate it into a new STARK definition, that is to say, a new pil, new constants and a starkinfo.
This translation, that we call \ctos (CIRCUIT-to-STARK), 
is performed also in a setup phase.
Following our example, the new generated STARK is denoted as
$\texttt{STARK}_{\texttt{B}}$ and it is essentially a \plonk{ish} arithmetization with some custom gates of the verification circuit of $\texttt{STARK}_{\texttt{A}}$
(for more details about \stoc see Section \ref{subsec:ctos}).
It is worth to mention that these recursion steps can be applied as many times as desired taking into account that each step will compress the proof making it
more efficient to verify but increasing the prover complexity.
Finally, remark that during the setup phase, several artifacts for generating each STARK prover are generated (see Sections \ref{subsec:stoc} and \ref{subsec:ctos} for more information 
about these artifacts). 

\subsubsection{Proving Phase \label{subsubsec:proving:phase:intro}}


The first proof is generated by providing the proper inputs and public values to the first STARK prover. Then, the output proof is passed as input to the next STARK prover together with the public inputs and the process is recursively repeated.
In Figure \ref{fig:recursive-provers} we show how in essence a chain of recursive STARK provers work.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{\recursiondir/figures/recursive-provers}
\caption{Recursive provers.}
\label{fig:recursive-provers}
\end{figure}

Notice that the final proof is actually a circuit-based proof (currently we are using a Groth16 proof). More details about the proving phase can be found in Section \ref{subsec:recursion:step:proof}.


\subsection{Aggregation} 

Our architecture also allows aggregation when generating the proofs. Aggregation is a particular type of proof composition in which multiple valid proofs can be all proven to be valid by comprising them all into one proof, called the \textit{aggregated proof}, and only validating the aggregated one.
In our architecture, aggregators are defined in intermediate circuits.
Figure \ref{fig:aggregation_example} shows an example of aggregation with binary aggregators.
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{\recursiondir/figures/radix-2-aggregation}
\caption{Aggregation example.}
\label{fig:aggregation_example}
\end{figure}


\subsection{Setup \stoc \label{subsec:stoc}}

Recall that we denote \stoc to the process of converting a given STARK into its verifier circuit, which is described in Circom and compiled to the corresponding R1CS constraints. The architecture of this generic conversion is depicted in Figure \ref{fig:setup-p2c}, where a \texttt{STARK}$_\texttt{x}$ is converted into a circuit denoted as $C_y$.


\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{\recursiondir/figures/setup-s2c}
\caption{Setup \stoc.}
\label{fig:setup-p2c}
\end{figure}

The input of the \stoc step is all the information needed to set up a circuit verifying the given STARK. 
In our architecture this is a PIL file specifying the STARK constraints that are going to be validated and the polynomial names, a \texttt{starkInfo} file containing the FRI-related parameters (blowup factor, the number of queries to be done, etc.), 
and the Merkle tree root of the computation constants (\texttt{constRoot}).
The output of the generate Circom process is a Circom description. 
The circuit is actually generated by filling an  \texttt{EJS} template
for the Circom description using the constraints defined by the PIL, the FRI-related parameters included by the \texttt{starkInfo} file and the \texttt{constRoot}. 
As shown in Figure \ref{fig:stark-verifier-circuit}, the inputs of the generated STARK verifier circuits are divided in two groups: the public inputs and the private inputs.

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{\recursiondir/figures/STARK-verifier-inputs}
\caption{Inputs of the STARK verifier circuits.}
\label{fig:stark-verifier-circuit}
\end{figure}

The private inputs are the parameters of the previous STARK proof:

%TODO: Marc -> All: Enable should be reviewed
%TODO: Jose -> All: review description with info from Roger

\begin{itemize}

\item \texttt{(rootC)}: Four field elements being the root of the Merkle tree for the evaluations of constant (that is, preprocessed) polynomials of the previous STARK.
In some of the intermediate circuits that we generate, \texttt{rootC} is an input of the circuit while in other generated circuits \texttt{rootC} are internal signals 
hardcoded to the corresponding values (more information for each particular case is provided later).

\item \texttt{root1}: Four field elements being the root of the Merkle tree for the evaluations of all the trace column polynomials for the execution trace. 

\item \texttt{root2}: Four field elements being the root of the Merkle tree for the evaluations of the $h$ polynomials appearing in each lookup argument of the previous STARK. This root may be $0$ if no lookup argument is provided in the PIL.  

\item \texttt{root3}: Four field elements being the root of the Merkle tree for the evaluations of the grand product polynomials appearing in each argument (that is, lookup, permutation or connection arguments) of the previous STARK and the intermediate polynomials appearing in certain splitting of them. This root may be $0$ if no arguments are provided in the PIL. 

\item \texttt{root4}: Four field elements being the root of the Merkle tree for the evaluations of the splitting $Q_1$ and $Q_2$ of the $Q$ polynomial of the previous STARK.

\item \texttt{evals}: Contains all the necessary evaluations for all the polynomials appearing in the FRI verification process at a challenge value $z$ and at $gz$. 

\item \texttt{si\_root}: Four field elements being their root of the Merkle tree for the evaluations of the $i$-folded FRI polynomial, that is, the polynomial appearing in the $i$-th step of the FRI verification. 

\item \texttt{si\_vals}: The leaves' values of the previous Merkle tree used to check all the queries. The total amount of such values depends on the number of queries and the reduction factor attached to the current step of the FRI. 


\item \texttt{si\_siblings}: Merkle proofs for each of the previous evaluations.

\item \texttt{finalPol}: Contains all the evaluations of the last step's folding polynomial constructed in the FRI verification procedure over the last defined domain which has the same size as the degree of the polynomial. 

\end{itemize}

The \texttt{publics} are a set of inputs that will be used by the verifier to check the final prove and also by the intermediate STARKs (more information about publics used in the zkEVM STARK is provided in Section \ref{subsec:zkEVM:architecture}).

The final process to complete the \stoc step is to compile the Circom description to obtain a file with the associated R1CS constraints and a witness calculator program capable of compute all the values of the circuit wires for a given set of inputs.

Finally, remark that the particular intermediate circuit generated in a \stoc step,  denoted as $C_y$ in Figure \ref{fig:setup-p2c}, can be just a verifier of the previous STARK (\texttt{STARK}$_\texttt{x}$ in Figure \ref{fig:setup-p2c}) if we are only applying a recursion step, but more generally, other types of circuits that include the verifier but provide more functionality can be used. 
This latter is the case when we use circuits to verify aggregation of proofs.


\subsection{Setup \ctos \label{subsec:ctos}}

In our proving architecture, we create a chain of STARKs. 
From the \stoc step we can obtain a circuit that has to be converted again 
into a STARK. 
The step that achieves this conversion is a pre-processing step 
that we call \ctos. A picture of a generic \ctos step can be found in Figure \ref{fig:setup-c2s}, where a circuit denoted as $C_y$
is converted into its corresponding STARK (\texttt{STARK}$_\texttt{y}$).

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{\recursiondir/figures/setup-c2s}
\caption{Setup recursion step \ctos.}
\label{fig:setup-c2s}
\end{figure}

In more detail, the STARK arithmetization of the intermediate circuits of our proving architecture is a \plonk{ish} arithmetization with custom gates and using 12 polynomials for the values of the gate wires of the computation trace.
The STARK arithmetization includes several custom gates for doing specific tasks more efficiently. In particular, the custom gates provide the following functionality:

\begin{itemize}
\item \textbf{Poseidon}: This custom gate is capable of validating a Poseidon hash from a given $8$ field elements as inputs, $4$ field elements as capacity and a variable number of output elements. More specifically, this circuit implements the MDS matrix and the $7$-th power of field elements computations used to execute each of the rounds defined by the Poseidon hash sponge construction. 
\item \textbf{Extended Field Operations}: This custom gate is capable of validating multiplications and additions (or a combination of both) over the extended field $\FF_{p^3}$. The inputs are $3$ elements $a, b, c \in \FF_{p^3}$ and the output is corresponds to the element 
\[
a \cdot b + c \in \FF_{p^3}
\]
where the operations are defined over $\FF_{p^3}$. Observe that defining $c$ equal to $0$ one can compute pure multiplications. Similarly, setting $b$ equal to $1$, pure additions can be computed. 

\item \textbf{FFT}: This custom gate is in charge of computing FFT of a variable size in $\FF_p$ or in an extension field. 

\item \textbf{Polynomial Evaluation}: This custom gate is in charge of computing a single evaluation of polynomials in $\FF_{p^3}$ using Horner's rule. The input consists on a field element $z \in \FF_{p^3}$ and the coefficients of the polynomial $p$ which we are going to evaluate. The output is the evaluation $p(z) \in \FF_{p^3}$. 
\end{itemize}

We use selector polynomials to activate the custom gates of our STARK. 
These polynomials are constant (pre-processed). In particular, we use the selectors \texttt{POSEIDON12}, \texttt{GATE}, \texttt{CMULADD}, \texttt{EVPOL4} and \texttt{FFT4} are introduced. The selector \texttt{GATE} is actually in charge of activating a basic \plonk gate. The other selectors are in charge of selecting whichever subcircuit needs to be executed. Moreover, there also exist a special selector called \texttt{PARTIAL} which is in charge of distinguishing between partial and full layers in the Poseidon, since this affects the relationships between the values.

At this point we can detail the processes of the \ctos step.
As shown in Figure \ref{fig:setup-c2s}, the first process of the \ctos step is the PIL setup, which takes the R1CS constraints of a given intermediate circuit as input and produces all the STARK-related artifacts.
This includes the associated STARK identity constraints and the computation constants
that are respectively stored in a PIL file (\texttt{y.pil}) and in a file of constants
(\texttt{y.const}).
In particular, the identity constraints of the \plonk{ish} arithmetization are generated by filling an \texttt{EJS} template for the associated PIL (for the zkEVM the template used is called \texttt{compressor12.pil.ejs}). 

The PIL setup also generates an important file with exec extension (\texttt{y.exec})
that defines how to rearrange the values produced by the circuit witness calculator into the appropriate values of the STARK execution trace (see Thaler's book for more information about this rearrangement process). 
Notice that the rearrangement rules and the computation constants only depend on 
the circuit shape (which is coded in the \texttt{.r1cs} file generated by the Circom compiler). In other words, these parameters do not depend on the particular values of the circuit wires computed for a particular input.
Nevertheless, we will use the rearrangement rules file together with the witness values for a given input later on to build the STARK execution trace, which in turn is needed to generate the STARK proof.

Finally, we also produce the \texttt{starkInfo} file and a Merkle tree with the
STARK constants.

\subsection{Recursion Step Proof \label{subsec:recursion:step:proof}}

As we explained in Section \ref{subsubsec:proving:phase:intro}, to generate the final proof, we have to compute the proof of each 
intermediate STARK (see Figure \ref{fig:recursive-provers}).
Each intermediate STARK proof is generated using the witness values provided by the execution of the associated circuit witness computation program using as inputs the publics and the values of the previous proof.
Then, these values are properly rearranged to build the STARK execution trace using the corresponding \texttt{.exec} file.
In Figure \ref{fig:recursion:step:proof}, we provide the scheme of how the proof of an intermediate STARK is generated. 

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{\recursiondir/figures/recursive-step-proof}
\caption{STARK \texttt{Proof} of a recursion step.}
\label{fig:recursion:step:proof}
\end{figure}

As it can be observed, the STARK executor process takes the parameters of previous proof and the public inputs (both in the file \texttt{x.zkin.proof} which
has the proper format for the witness calculator generated by the Circom compiler), the PIL of the current STARK (\texttt{y.pil}), the witness calculator program of the associated circuit (\texttt{y.witnesscalc}) and the file of rearrangement rules (\texttt{y.exec}) to generate the non-preprocessed part of the STARK execution trace (\texttt{y.commit}). 
Next, the STARK prover process takes the execution trace, that is to say, the committed and constant polynomials, the constant tree, the corresponding PIL file and the information provided by the \texttt{zkevm.starkinfo.json} file to generate the proof.
Finally, when the proof is generated, the STARK prover process generates three files:

\vspace{0.1cm}
\begin{compactitem}
\item \textbf{Proof File} (\texttt{y.proof.json}): A json file containing the whole STARK proof in a \texttt{.json} file. 

\item \textbf{Publics File} (\texttt{y.public.json}): A json file containing only the publics. 

\item \textbf{zkIn File} (\texttt{y.zkin.proof.json}): A json file combining both the proof and the publics. 
\end{compactitem}


\section{Polygon zkEVM \label{section:polygon:zkevm}}

\subsection{Architecture \label{subsec:zkEVM:architecture}}

In this section, we provide the concrete blocks and steps used to
prove the correct execution of a batch of transactions (or several batches)
by our zkEVM using recursion, aggregation and composition.
As previously mentioned, generating a proof has two phases. 
The first phase is a setup executed only once per STARK computation definition. 
In our case, the STARK computation is the processing of batches by our zkEVM.  
In the setup phase, the different artifacts needed to generate proofs are preprocessed.
The second phase is when actually proofs are generated for given inputs (i.e. batches of transactions).
An overview of the overall process can be observed in figure \ref{fig:architecture-aggregation-recursion-composition}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\recursiondir/figures/recursive-diagram-groth}
\caption{Proving architecture with recursion, aggregation and composition.}
\label{fig:architecture-aggregation-recursion-composition}
\end{figure}

Recall that the first STARK generates such a big proof since it has a lot of polynomials so its attached FRI uses a low blowup factor. Henceforth, a first \textit{Compression Stage} its invoked in each batch's proof, aiming to reduce the number of polynomials used, allowing to augment the blowup factor and therefore, reduce the proof size.  

Once the compression step has been completed, a proof aggregation stage will be in charge of joining several batches proofs into a single proof proving each of the single proofs all at once. The way of proceeding will be to construct a binary tree of proofs by aggregating two by two each of them. We will call this the \textit{Aggregation Stage}. 


However, since the aggregation of two proofs requires the constant root of the previous circuits through a public input coming from the previous circuit, there exists a \textit{Normalization Stage} which is in charge of transforming the obtained verifier circuit verifying the \texttt{c12a} proof into a one making the constant root public to the next circuit. This step allows each aggregator verifiers and the normalization verifiers to be exactly the same, permitting successful aggregation via a recursion. 

Once the normalization step has been finished, its time for aggregation. In this step we are going to join two batches' proofs together, which will be done many times until only one proof spares. In order to do so, a circuit capable of aggregating two verifiers is created. However, as we can see in the figure below, the inputs of this stage can be proofs of the kind $\pi_{\text{re1}}$ coming from the previous normalization stage, or  already aggregated proofs $\pi_{\text{re2}}$. This allows us to aggregate two $\pi_{\text{re1}}$ proofs, two $\pi_{\text{re2}}$ proofs or a combination of a $\pi_{\text{re1}}$ and a $\pi_{\text{re2}}$ proofs. Henceforth, this aggregation stage has to take into account this fact in its design. 


Observe that the \textit{Aggregation Stage} needs to be designed in order to accept either already aggregated proofs or only compressed ones. 
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{\recursiondir/figures/recursive-diagram-aggregation}
%\caption{Aggregation of proofs.}
%\label{fig:recursive-aggregation-intro}
%\end{figure}
The \textit{Final Stage} is the very last STARK step among the recursion process, which is in charge of verifying a $\pi_{\text{re2}}$ proof over a completely different finite field, the one defined by the \texttt{bn128} elliptic curve. More specifically, the hash in charge of generating the transcript will work over the field of the \texttt{bn128} curve. Hence, all the challenges (and so, all polynomials) will belong to this new field. This is done in this way because, in the next step of the process, a SNARK Groth16 proof, which works over elliptic curves, will be generated. In this step is very similar to the others, instantiating a verifier circuit for $\pi_{\text{re2}}$ but, in this case, $2$ constants roots should be provided (one for each of the proofs aggregated in the former step). 


%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{\recursiondir/figures/recursive-diagram-final}
%\caption{Final stage of the recursion.}
%\label{fig:recursivef-stage}
%\end{figure}

The last step of the whole process is called \textit{SNARK Stage}, and it purpose is to produce a Groth16 proof $\pi_{\text{groth16}}$ verifying the previous $\pi_{\text{ref}}$ proof. In fact, Groth16 can be replaced with any other SNARK proof. A SNARK is chosen here in order to reduce both verification complexity and proof size, which have constant complexity unlike STARK proofs. The $\pi_{\text{groth16}}$ proof will be sent to the verifier so that he/she can verify it. 


As a final remark, one should observe that the whole set of public inputs are being passed as inputs in each proof of the whole recursion procedure. The set of all public inputs is listed below (see the technical documents about the zkEVM L2 state management and the bridge):

\vspace{0.15cm}
\begin{compactitem}

\item \texttt{oldStateRoot}

\item \texttt{oldAccInputHash}

\item \texttt{oldBatchNum}

\item \texttt{chainId}

\item \texttt{midStateRoot}

\item \texttt{midAccInputHash}

\item \texttt{midBatchNum}

\item \texttt{newStateRoot}

\item \texttt{newAccInputHash}

\item \texttt{localExitRoot}

\item \texttt{newBatchNum}
\end{compactitem}

Next, let's describe the details of the steps and processes performed in each phase.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Setup Phase}

The setup phase is a pre-processing phase in which all the artifacts for generating proofs are created. This includes the generation of intermediate circuits, which 
are a finite set of circuits that allow arbitrary combinations of proof recursions and proof aggregations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Build the zkEVM STARK}

As shown in Figure \ref{fig:zkevm:build}, we start by building the ROM of the zkEVM state machine, which is the program containing the instructions for the the executor that will generate the execution trace of the zkEVM. We also build the PIL that validates the execution trace. The ROM will, in fact, generate all the constant values for the execution trace of the zkEVM. Observe that, as said before, committed polynomials are not needed in the setup phase, so we need not run the executor of the zkEVM in order to generate them at this point. 

%TODO mirar que parte del starkInfo hace falta para generar el circom.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{\recursiondir/figures/zkevm-build}
\caption{Build the zkEVM STARK}
\label{fig:zkevm:build}
\end{figure}

The Merkle root of the tree of constant polynomials' evaluations, which is a hash that serves as cryptographic summary to capture all the fixed parameters of the computation, is stored as a parameter in a file called \texttt{zkevm.verkey}.
The last piece of data that is generated before building the STARK is the \texttt{starkInfo} that is necessary for automatically generating the circuit that will verify the zkEVM STARK. 
In this case, we use a blowup factor of $2$ and $128$ queries to generate the proof. 
The artifacts marked in gray will be used when generating the proof (proof generation is described in more detail in Section \ref{subsec:proof:gen:phase}). 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Setup \stoc for the zkEVM STARK}

The next step in the setup is to generate the circuit to verify the zkEVM STARK (see Figure \ref{fig:zkevm:s2c}). 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{\recursiondir/figures/zkevm-s2c}
\caption{Converting the zkEVM STARK verification into a circuit.}
\label{fig:zkevm:s2c}
\end{figure}


The \texttt{pil2circom} process fills an \texttt{EJS} template called \texttt{stark\_verifier.circom.ejs} with all the necessary information needed to validate the zkEVM STARK. Henceforth, we need to add the \texttt{zkevm.pil} in order to capture polynomial names, the \texttt{zkevm.starkinfo} file which specifies the blowup factor, the number of queries and the steps of the FRI-verification procedure and the \texttt{constRoot} in the \texttt{zkevm.verkey} file to automatically generates a circuit in Circom. The output Circom file \texttt{zkevm.verifier.circom} is then compiled into R1CS constraint system written in a file called \texttt{zkevm.verifier.r1cs}. This constraints will be used in the next step to generate the PIL and the constant polynomials for the next proof. 

On the other hand, the Circom compilation also outputs a witness calculator program that we call \texttt{zkevm.verifier.witnesscalc}. As it can be observed in the picture, the witness calculator program is marked in gray because it is going to be used when generating the proof.

Since our aim at the next proof generation will be compression (that is, proof size reduction) we will use a blowup factor of $4$ in this step, with $64$ queries. This information is contained in the \texttt{c12a.starkstruct} file located in the \texttt{proverjs} repository.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Setup \ctos \texttt{c12a}}

The circuit that verifies the zkEVM STARK is called \texttt{zkevm.verifier} (or also \texttt{c12a}). This is because the PIL that is going to verify the \texttt{c12a} circuit is a \plonk{ish} circuit with custom gates and 12 polynomials aiming at compression. 

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{\recursiondir/figures/c12a-c2s}
\caption{Convert the zkEVM verifier circuit to a STARK called \texttt{c12a}.}
\label{fig:zkevm-verifier-c2s}
\end{figure}

From the previous R1CS description of the verification circuit we are going to obtain a machine-like construction whose correct execution, which will be described by a PIL file, will be equivalent to the validity of the previous circuit. This process is started through a service called \texttt{compressor12\_setup}, where the corresponding PIL file for verifying the trace is output, as well as a binary for all the constant polynomials \texttt{c12a.const} defined by it. Moreover, a helper file called \texttt{c12a.exec} is generated by the same service. This helper file will contain all the necessary rules that will allow us to shuffle all the witness values, which will be computed later on, into the corresponding position of the execution trace. The design of this shuffling, together with the the connections defined in the constants polynomials \texttt{c12a.const} will ensure that, for a honest prover, this newly generated trace is valid whenever the previous circuit is. 

In order to finish the set up phase, having all the FRI-related parameters willing to be used in this step stored in a \texttt{c12a.starkstruct} file (located in the prover repository), we can generate the \texttt{c12a.starkinfo} file through the \texttt{generate\_starkinfo} service and build the constants' tree (and its respectively constant root). 




\subsubsection{Setup \stoc for \texttt{recursive1}}

Up to this point we have a STARK proof $\pi_{\text{c12a}}$ verifying the first big STARK proof $\pi$. The idea now is, as before, generate a Circom circuit that verifies $\pi_{\text{c12a}}$ by miming the FRI verification procedure. To do that, we generate a verifier circuit \texttt{c12a.verifier.circom} from the previously obtained \texttt{c12a.pil} file, the \texttt{c12a.starkinfo} file and the constant root \texttt{c12a.verkey.constRoot} by filling the \texttt{stark\_verifier.circom.ejs} template as before.

In this case, in seek of normalization, we need to briefly modify this circuit in order to include the constant root as a public input. Observe that, since we are not still aggregating, the constant root will not be actually used here, though this will be extremely important in the aggregation stage, where all the constants for the computation, which depend on the previous circuit, need to be provided as public inputs. This is done by using \texttt{recursive1.circom} file and importing inside the previously generated \texttt{c12a.verifier.circom} circuit as a library. The verifier circuit is instantiated inside \texttt{recursive1.circom}, connecting all the necessary wires and including the constant root to the set of publics. 

The output circom file \texttt{recursive1.circom} it is compiled into a R1CS \texttt{recursive1.r1cs} file and a witness calculator program \texttt{recursive1.witnesscal} which will be both used later on in order to build and fill the next execution trace. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\recursiondir/figures/c12a-s2c}
\caption{Convert the c12a STARK to a c12a verifier circuit.}
\label{fig:c12a-s2c}
\end{figure}

\subsubsection{Setup \ctos for \texttt{recursive1}}

As before, from the previous R1CS description of the verification circuit we are going to obtain a machine-like construction whose correct execution, which will be described by a PIL \texttt{recursive1.pil} file, will be equivalent to the validity of the previous circuit. Also, a binary for all the constant polynomials \texttt{recursive1.const} defined by it and the helper file providing the witness values allocation into its corresponding position of the execution trace \texttt{recursive1.exec} are generated.

In order to finish the set up phase, having all the FRI-related parameters willing to be used in this step stored in a \texttt{recursive.starkstruct} file (located in the prover repository), we can generate the \texttt{recursive1.starkinfo} file through the \texttt{generate\_starkinfo} service and build the constants' tree (and its respectively constant root). In this case, we are using a blowup factor of $2^4 = 16$, allowing the number of queries to be $32$. 

\begin{figure}[H]
\centering
\includegraphics[width=.85\textwidth]{\recursiondir/figures/recursive1-c2s}
\caption{Convert the \texttt{recursive1} circuit to its associated STARK.}
\label{fig:recursive1-c2s}
\end{figure}



\subsubsection{Setup \stoc for \texttt{recursive2}}

Up to this point we have a STARK proof $\pi_{\text{re1}}$ verifying the proof $\pi_{\text{c12a}}$. As before, we generate a Circom circuit that verifies $\pi_{\text{re1}}$ by miming the FRI verification procedure. To do that, we generate a verifier circuit \texttt{recursive1.verifier.circom} from the previously obtained \texttt{recursive1.pil} file, the \texttt{recursive1.starkinfo} file and the constant root \texttt{recursive1.verkey.constRoot} by filling the verifier \texttt{stark\_verifier.circom.ejs} template.

After the verifier is generated using the template, we will also use a template to create another Circom that aggregates two verifiers. Note that, in the previous step, the constant root is passed harcoded into the circuit from an external file. But this was intended as a normalization, allowing the previous circuit and each ones verifying each of both proofs to have exactly the same form, allowing recursion being possible. Henceforth, this \texttt{recursive2.circom} circuit has two verifiers and a two multiplexors that are actually deciding the form of each of the verifiers: if the proof has $\pi_{\text{re1}}$ form, the hardcoded constant root is input but, if the proof has $\pi_{\text{re2}}$ form, the constant root should be connected as input signal, coming from a previous circuit. 

A schema of the \texttt{recursive2} circuit generated is as shown in Figure \ref{fig:recursive2-circuit}. Observe that, since the upper proof has the $\pi_{\text{re2}}$ form, the Multiplexor does not provides the constant root \texttt{rootC} to the Verifier A to hardcode it because this verifier should get it trough a public input from the previous circuit. Otherwise, the lower proof has the $\pi_{\text{re1}}$ form, so the Multiplexor let is pass trough the constant root into the Verifier B so that it can be hardcoded it when the corresponding template is filled. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{\recursiondir/figures/recursive2-circuit}
	\caption{\texttt{recursive2} circuit.}
	\label{fig:recursive2-circuit}
\end{figure}


The output circom file \texttt{recursive2.circom}, obtained running a different script called \texttt{genrecursive} it is compiled into a R1CS \texttt{recursive2.r1cs} file and a witness calculator program \texttt{recursive2.witnesscal} which will be both used later on in order to build and fill the next execution trace. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\recursiondir/figures/recursive1-s2c}
\caption{Convert the \texttt{recursive1} STARK to its verifier circuit called \texttt{recursive2}.}
\label{fig:recursive1-s2c}
\end{figure}




\subsubsection{Setup \ctos for \texttt{recursive2}}

As before, from the previous R1CS description of the verification circuit we are going to obtain a machine-like construction whose correct execution, which will be described by a PIL \texttt{recursive2.pil} file, will be equivalent to the validity of the previous circuit. Also, a binary for all the constant polynomials \texttt{recursive2.const} defined by it and the helper file providing the witness values allocation into its corresponding position of the execution trace \texttt{recursive2.exec} are generated.

In order to finish the set up phase, having all the FRI-related parameters willing to be used in this step stored in a \texttt{recursive.starkstruct} file (located in the prover repository), we can generate the \texttt{recursive2.starkinfo} file through the \texttt{generate\_starkinfo} service and build the constants' tree (and its respectively constant root). Since we want both verifiers to be exactly the same as in the previous step, we are using the same blowup factor of $2^4 = 16$, allowing the number of queries to be $32$. 

\begin{figure}[H]
	\centering
	\includegraphics[width=.8\textwidth]{\recursiondir/figures/recursive2-c2s}
	\caption{Convert the \texttt{recursive2} circuit to its associated STARK.}
	\label{fig:recursive2-c2s}
\end{figure}



\subsubsection{Setup \stoc for \texttt{recursivef}}

Up to this point we have a STARK proof $\pi_{\text{re2}}$ verifying another  $\pi_{\text{re2}}$ proof (or $\pi_{\text{re1}}$ in some edge cases). The idea now is, as before, generate a Circom circuit that verifies $\pi_{\text{re2}}$ (or $\pi_{\text{re1}}$ if no aggregation is taking place) by miming the FRI verification procedure as done before. To do that, we generate a verifier circuit \texttt{recursive2.verifier.circom} from the previously obtained \texttt{recursive2.pil} file, the \texttt{recursive2.starkinfo} file and the constant roots of the previous two proofs \texttt{recursive2\_a.verkey.constRoot} and \texttt{recursive2\_b.verkey.constRoot} by filling the \texttt{stark\_verifier.circom.ejs} template as before.

The output circom file \texttt{recursivef.circom}, obtained running a different script called \texttt{genrecursivef} it is compiled into a R1CS \texttt{recursivef.r1cs} file and a witness calculator program \texttt{recursivef.witnesscal} which will be both used later on in order to build and fill the next execution trace. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\recursiondir/figures/recursive2-s2c}
\caption{Convert the \texttt{recursive2} STARK to its verifier circuit called \texttt{recursivef}.}
\label{fig:recursive2-s2c}
\end{figure}

\subsubsection{Setup \ctos for \texttt{recursivef}}

As before, from the R1CS description of the verification circuit we are going to obtain a machine-like construction whose correct execution, which will be described by a \texttt{recursive2.pil} PIL file, will be equivalent to the validity of the previous circuit. Also, a binary for all the constant polynomials \texttt{recursive2.const} defined by it and the helper file providing the witness values allocation into its corresponding position of the execution trace \texttt{recursive2.exec} are generated.

In order to finish the set up phase, having all the FRI-related parameters willing to be used in this step stored in a \texttt{recursivef.starkstruct} file (located in the prover repository), we can generate the \texttt{recursivef.starkinfo} file through the \texttt{generate\_starkinfo} service and build the constants' tree (and its respectively constant root). 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{\recursiondir/figures/recursivef-c2s}
\caption{Convert the recursivef circuit to its associated STARK.}
\label{fig:recursivef-c2s}
\end{figure}




\subsubsection{Setup \stoc for \texttt{final}}

Up to this point we have a STARK proof $\pi_{\text{ref}}$ verifying a proof $\pi_{\text{re2}}$. As before, we generate a Circom circuit that verifies $\pi_{\text{ref}}$ by miming the FRI verification procedure. To do that, we generate a verifier circuit \texttt{recursivef.verifier.circom} from the previously obtained \texttt{recursivef.pil} file, the \texttt{recursivef.starkinfo} file and the constant root \texttt{recursivef.verkey.constRoot} by filling the \texttt{stark\_verifier.circom.ejs} verifier template. This verifier Circom file will be imported by the \texttt{final.circom} circuit in order to generate the circuit that will be proven using Groth16 procedure. 



\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{\recursiondir/figures/final-s2c}
	\caption{Convert the \texttt{recursivef} STARK to its verifier circuit that is called \texttt{final.circom}.}
	\label{fig:final-s2c}
\end{figure}



\subsection{Proof Generation Phase \label{subsec:proof:gen:phase}}

\subsubsection{Proof of the zkEVM STARK}

Up to this point we built an execution trace together with a PIL file describing the ROM of the zkEVM. Having both we can generate a STARK proof stating the correct execution of the zkEVM using the \texttt{pil-stark} tooling explained in section \ref{subsec:non_recursive_STARK}. In this step, a blowup factor of $2$ is used, so the proof having a huge amount of polynomials becomes quite big. To amend that, the next step \texttt{c12a} will be a compression step, raising the blowup factor and aiming to reduce the number of polynomials. 

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{\recursiondir/figures/zkevm-proof}
\caption{Generation for a zkEVM Proof}
\label{fig:recursive-zkemv-proof}
\end{figure}

To generate the proof, \texttt{main\_prover} service is used. The service requires to provide the execution trace (that is, the committed and constant polynomials files generated by the executor using the \texttt{pilcom} package), the constant tree binary file in order to be hashed to construct the constant root, the PIL file of the zkEVM ROM \texttt{zkev.pil} and all the information provided by the \texttt{zkevm.starkinfo.json} file, including all the FRI-related parameters such as the blowup factor or the configuration of the steps. 

This step differs from the next ones as it is the first and it is intended to start the recursion. However, aiming uniformization code-wise,  the Main Prover procedure choose to abstract the notion of proving and is intended to be the same at each step among the recursion. 



\subsubsection{Proof of \texttt{c12a}}


%TODO: Jose: The publics of c12a are the same as zkevm right?

To generate the proof verifying the previous \texttt{zkevm.proof}, we generate all the witness values and map them correctly into its corresponding position of the execution trace exactly in the same way as before, obtaining a binary file \texttt{ca12.commit} for the committed polynomials of the execution trace. 

Having the execution trace (that is, the committed and constant polynomials filled) and the PIL, we can generate a proof validating the previous big STARK proof. We use the same service \texttt{main\_prover} as before to do it, providing also the previously built constant tree \texttt{ca12.constTree} and the \texttt{ca12.starkinfo} file. This will generate the proof and the publics joined in the \texttt{c12a.zkin.proof} file.  

\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{\recursiondir/figures/c12a-proof}
\caption{Generate a STARK proof for \texttt{c12a}.}
\label{fig:c12a-proof}
\end{figure}



\subsubsection{Proof of \texttt{recursive1}}

To generate the proof verifying the previous \texttt{c12a.proof}, we generate all the witness values and map them correctly into its corresponding position of the execution trace exactly in the same way as before, obtaining a binary file \texttt{recursive1.commit} for the committed polynomials of the execution trace. 

Having the execution trace (that is, the committed and constant polynomials filled) and the PIL, we can generate a proof validating the previous big STARK proof. We use the same service \texttt{main\_prover} as before to do it, providing also the previously built constant tree \texttt{recursive1.constTree} and the \texttt{recursive1.starkinfo} file. This will generate the proof and the publics joined in the \texttt{recursive1.zkin.proof} file. 

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{\recursiondir/figures/recursive1-proof}
\caption{Generate a STARK proof for \texttt{recursive1}.}
\label{fig:recursive1-proof}
\end{figure}



\subsubsection{Proof of \texttt{recursive2}}

To generate the proof verifying the previous \texttt{recursive1.proof}, we generate all the witness values and map them correctly into its corresponding position of the execution trace exactly in the same way as before, obtaining a binary file \texttt{recursive2.commit} for the committed polynomials of the execution trace. 

Having the execution trace (that is, the committed and constant polynomials filled) and the PIL, we can generate a proof validating the previous big STARK proof. We use the same service \texttt{main\_prover} as before to do it, providing also the previously built constant tree \texttt{recursive2.constTree} and the \texttt{recursive2.starkinfo} file. This will generate the proof and the publics joined in the \texttt{recursive2.zkin.proof} file. 

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{\recursiondir/figures/recursive2-proof}
\caption{Generate a STARK proof for \texttt{recursive2}.}
\label{fig:recursive2-proof}
\end{figure}


\subsubsection{Proof of \texttt{recursivef}}

To generate the proof verifying the previous \texttt{recursive2.proof}, we generate all the witness values and map them correctly into its corresponding position of the execution trace exactly in the same way as before, obtaining a binary file \texttt{recursivef.commit} for the committed polynomials of the execution trace. 

Having the execution trace (that is, the committed and constant polynomials filled) and the PIL, we can generate a proof validating the previous big STARK proof. We use the same service \texttt{main\_prover} as before to do it, providing also the previously built constant tree \texttt{recursivef.constTree} and the \texttt{recursivef.starkinfo} file. This will generate the proof and the publics joined in the \texttt{recursivef.zkin.proof} file. 


\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{\recursiondir/figures/recursivef-proof}
\caption{Generate a STARK proof for \texttt{recursivef}.}
\label{fig:recursivef-proof}
\end{figure}



\subsubsection{Proof of \texttt{final}}

The last circuit, \texttt{final.circom} is the one used to generate the proof.
At this moment a Groth16 proof is generated.


\section{Building zkEVM Proofs}

The setup phase runs with the proverjs. 
The circuits build in the setup phase can be used as many times
as desired.
In production, the proof generation runs with the prover written in C++.
The prover receives the information about the particular composition of 
proofs via an gRPC API.
Next we explain how to use each software component in more detail.

\subsection{Preprocessing and Initialitiation}
%https://hackmd.io/6HgHAvgjR-CB08UaU5MsjA?view


\subsubsection{Requirements}

In order to verify the Smart Contract, there will be necessary a machine with at least $256$GB of RAM and $16$ cores. This section's tutorial will give instructions for a \texttt{r6a.8xlarge} AWS instance, with $16$ cores, $32$ threads and $512$GB of SSD. This instance uses Ubuntu 22.04 LTS costing $1.82$ dollars per hour. 


\subsubsection{Setup}

First of all, we ought prepare the OS in order to be able to run the verification procedure: we update the apt packages and install \texttt{tmux}, \texttt{git} and \texttt{curl}.

\begin{lstlisting}[style=termt]
sudo apt update
sudo apt install -y tmux git curl
\end{lstlisting}

Posteriorly, the kernel configuration parameters should be changed in order to accept high amount of memory:

\begin{lstlisting}[style=termt]
echo "vm.max_map_count=655300" | sudo tee -a /etc/sysctl.conf
sudo sysctl -w vm.max_map_count=655300
export NODE_OPTIONS="--max-old-space-size=230000"
\end{lstlisting}

Now, we install the \texttt{NodeSource} \texttt{Node.js} \texttt{18.x} repository onto our Ubuntu machine. \textbf{It is important to check that the main version of Node is \texttt{18}. }

\begin{lstlisting}[style=termt]
curl -sL "https://deb.nodesource.com/setup_18.x" -o nodesource_setup.sh
sudo bash nodesource_setup.sh
sudo apt install -y nodejs
node -v
\end{lstlisting}

Moreover, in order to be able to compile circuits involved, we need \texttt{circom} installed: 

\begin{lstlisting}[style=termt]
cd ~
git clone https://github.com/iden3/circom.git
cd circom
git checkout v2.1.4
git log --pretty=format:'%H' -n 1
\end{lstlisting}

The hasth of the commit should be 
\[
\texttt{ca3345681549c859af1f3f42128e53e3e43fe5e2.}
\]

Now, we install and compile \texttt{cicom} using Rust's package manage and build system: \texttt{cargo}. It is important to check that the version of \texttt{circom} is \texttt{2.1.4}:

\begin{lstlisting}[style=termt]
sudo apt install -y cargo
cargo build --release
cargo install --path circom
export PATH=$PATH:~/.cargo/bin
echo 'PATH=$PATH:~/.cargo/bin' >> ~/.profile
circom --version
\end{lstlisting}

In order to build the tree of constants in a fast way we have integrated a tool called \texttt{bctree}. We can build it using the following set of commands:

\begin{lstlisting}[style=termt]
cd ~
git clone https://github.com/0xPolygonHermez/zkevm-prover.git
cd zkevm-prover
git checkout b9179f3b870c2922b11d09462b447d0fc35dd2f9
git submodule init
git submodule update
sudo apt install -y build-essential libomp-dev libgmp-dev nlohmann-json3-dev libpqxx-dev nasm libgrpc++-dev libprotobuf-dev grpc-proto libsodium-dev uuid-dev libsecp256k1-dev
make -j bctree
\end{lstlisting}

This step takes approximately $8$ minutes to complete.



\subsubsection{Preprocessing}

The next step consists on building all the setup steps specified before in this document, such as building all constants and circuits. This can be done using the code below. Observe that we are actually using the previously built \texttt{bcee} tool above in the last command, under the \texttt{--bctree} flag.

\begin{lstlisting}[style=termt]
cd ~
git clone https://github.com/0xPolygonHermez/zkevm-proverjs.git
cd zkevm-proverjs
git checkout 59694e4a8a9358772c23e8110f53aee049e2d5bd
npm install
tmux -c "npm run buildsetup --bctree=../zkevm-prover/build/bctree"
\end{lstlisting}

This step is quite long, it takes approximately 4.5 hours. 2 out of 4.5 hours are for the \texttt{powersOfTau28\_hez\_final.ptau} download, a file of 288GB that it’s loaded only once.

\subsection{Verification of the Smart Contract}

As a final result of the previous steps, the smart contract that verifies the test has been generated. This file is \texttt{final.fflonk.verifier.sol}. At this point, it is possible to verify the smart contract using the source code or verify that the bytecode is the same. 

\subsubsection{Option 1: Verify the Smart Contract using the Source Code}

To verify the bytecode, you must compile with the precisely same version, compiler, and parameters to be sure that even the metadata hash contained in the bytecode is exactly the same. The following instructions generate a project to build using the hardhat tool.

\begin{lstlisting}[style=termt]
cd ~
mkdir contract
cd contract
npm init -y 
npm install hardhat
mkdir -p contracts/verifiers
echo -e "module.exports={solidity:{compilers:[{version: \"0.8.17\",settings:{optimizer:{enabled:true,runs:999999}}}]}}" > hardhat.config.js
\end{lstlisting}

Once the project structure is created, we proceed to copy the smart contract generated in the previous step. This smart contract was saved on \path{~ /zkevm-proverjs/build/proof}, and must be copied to \texttt{contracts/verifiers} with exactly the name \texttt{FflonkVerifier.sol}. If the name or the path changes, the hash of metadata changes too, for this reason, is essential to respect the exact name and path. This can be done by executing these commands:

\begin{lstlisting}[style=termt]

cp ~/zkevm-proverjs/build/proof/final.fflonk.verifier.sol contracts/verifiers/FflonkVerifier.sol
sha256sum contracts/verifiers/FflonkVerifier.sol
\end{lstlisting}

We should double check that the output hash digest for the \texttt{FflonkVerifier.sol} file is exactly the one shown below:
\[
\texttt{60a850bd73a316d6f85bfdf05bc69ee01f84d889004e98b07847719f59e49761.}
\]

Once this is done and check, we can compile the Smart Contract using the following command:

\begin{lstlisting}[style=termt]
npx hardhat compile
\end{lstlisting}

The bytecode of the Smart Contract was on the \texttt{bytecode} property of the \texttt{.json} file \texttt{FflonkVerifier.json} generated on path \texttt{artifacts/contracts/verifiers/FflonkVerifier.sol}:

\begin{lstlisting}[style=termt]
608060405234801561001057600080fd5b50612a
07806100206000396000f3fe6080604052348015
61001057600080fd5b506004361061002b576000
:
:
017fffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffe0908116603f01
1681019083821181831017156129875761298761
28aa565b81604052828152886020848701011115
6129a057600080fd5b8260208601602083013760
006020848301015280965050505050506129c884
602085016128d9565b9050925092905056fea264
6970667358221220f7f0438c1c741f762e101c4e
ae732373046d40de3d0f2e0808e1fac1d863c78f
64736f6c63430008110033
\end{lstlisting}

The following command allows us to extract previous bytecode in one line of a file \texttt{FflonkVerifier.sol.compiled.bytecode}:

\begin{lstlisting}[style=termt]
grep bytecode artifacts/contracts/verifiers/FflonkVerifier.sol/FflonkVerifier.json |sed 's/.*"\(0x.*\)".*/\1/' > FflonkVerifier.sol.compiled.bytecode
\end{lstlisting}

If you prefer you can copy by hand the content of the bytecode of the file
\[
\texttt{artifacts/contracts/verifiers/FflonkVerifier.sol/FflonkVerifier.json}
\]
to the file \texttt{FflonkVerifier.sol.compiled.bytecode}. Remember to copy only the content inside the double quotes (without double quotes).

Then, use the hash-digest to verify integrity of the compiled bytecode

\begin{lstlisting}[style=termt]
sha256sum FflonkVerifier.sol.compiled.bytecode
\end{lstlisting}

The expected result is the following one
\[
\texttt{44f70b6a1548f49214027169db477f03e92492f19ac5594c648edb478316cbec.}
\]


\subsubsection{Option 2: Verify the Smart Contract comparing the Bytecode}

To download bytecode of deployed Smart Contract we need his address. In this case it’s \texttt{0x4ceB990D2E2ee6d0e163fa80d12bac72C0F28D52}. Then, we can direct to Etherscan, Blockscout or Beaconcha to get transaction bytecode.

Some applications running on the terminal may limit the amount of input they will accept before their input buffers overflow. To avoid this situation create file \path{FflonkVerifier.sol.explorer.bytecode} with an editor such as \texttt{nano} or \texttt{vi}.

\begin{lstlisting}[style=termt]
cd ~/contract
nano FflonkVerifier.sol.explorer.bytecode
\end{lstlisting}

Then, paste the clipboard to the file and compare that the two files providing to both Option 1 and Option 2 are actually the same. We can do it using a \texttt{diff}:

\begin{lstlisting}[style=termt]
cd ~/contract
diff FflonkVerifier.sol.compiled.bytecode FflonkVerifier.sol.explorer.bytecode
\end{lstlisting}


Or using a hash file integrity verification method:

\begin{lstlisting}[style=termt]
cd ~/contract
sha256sum FflonkVerifier.sol.*.bytecode
\end{lstlisting}

The output of the previous command in the terminal should be

\begin{lstlisting}[style=termt]
44f70b6a1548f49214027169db477f03e92492f19ac5594c648edb478316cbec  FflonkVerifier.sol.compiled.bytecode
44f70b6a1548f49214027169db477f03e92492f19ac5594c648edb478316cbec  FflonkVerifier.sol.explorer.bytecode
\end{lstlisting}


\subsection{zkEVM Proving}


\subsubsection{Prover JS}

The \texttt{main\_executor} script located in the \texttt{src} folder is used to generate the committed polynomials \texttt{commit.bin} of the execution trace for a compiled \texttt{rom.json} file coming from a \texttt{rom.zkasm} file by the zkASM compiler using a specific set of inputs from a file \texttt{input.json}. The common use of this script is shown below:

\begin{lstlisting}[style=termt]
node src/main_executor <input.json> -r <rom.json> -o <commit.bin>
\end{lstlisting}

Below, we list all the possible additional parameters that can be added to modify its usage:

\begin{itemize}


\item \path{-t <test.json>}: Test.

\item \path{-l <logs.json>}: Output for logs.

\item \path{-s}: Skip \texttt{.pil} file compilation.

\item \path{-d}: Debug mode.

\item \path{-p}: select a concrete PIL program. For example, \texttt{pilprogram.pil}.

\item \path{-P}: Load \texttt{pilConfig.json} file.

\item \path{-u}: Unsigned transactions mode.

\item \path{-e}: Skip asserts \texttt{newStateRoot} and \texttt{newLocalExitRoot}.

\item \path{-v}: Verbose mode.


\end{itemize}


The \texttt{buildall} command is used as a main entrypoint to all the build setup steps specified in the previous sections. 

\begin{lstlisting}[style=termt]
npm run buildall 
\end{lstlisting}

Below, we describe how to use it in full detail, listing all the involved flags:

\begin{itemize}

\item \path{--pil=<pil_file.pil>}: This option specifies the name of the PIL file to compile. For example, \path{--pil=src/main.pil}.

\item \path{--pilconfig=<pilconfig.json>}: This option specifies the name of the PIL configuration file to use. The PIL configuration file contains various settings and options that affect the compilation process. For example, \path{--pilconfig=config/pilconfig.json}.

\item \path{--starkstruct=debug}: This option generates an automatic STARK struct adapted to the PIL bits number. This option is typically used for debugging purposes. 

\item \path{--from=<step>}: This option specifies the step at which to start the build/rebuild process. For example, \path{--from=c12setup}.

\item \path{--continue}: This option continues the build process from where it left off during the previous build.

\item \path{--step=<step>}: This option specifies single specific build step to execute. For example, \path{--step=c12setup}.

\item \path{--build=<build_dir>}: This option specifies the name of the build directory to use. The build directory is where the compiled output files are stored. For example, \path{--build=build/basic_proof}.

\item \path{--input=<input_file>}: This option specifies the name of the input file to use for the build process. For example, \path{--input=test/myinputfile.json}.

\end{itemize}

\subsubsection{Prover C++}

The code and the instructions to compile and run the zkEVM Prover can be found at \href[]{https://github.com/0xPolygonHermez/zkevm-prover}{0xPolygonHermez/zkevm-prover} GitHub repository.
The zkEVM Prover process can provide up to three RPC services. 

The \textbf{Executor service} calls the Executor component of the zkEVM Prover to generate an execution trace for a set of L2 transactions.
The execution trace can be used to prove the new state resulting after executing all the involved transactions. 
However, this service does not generate the proof, just the execution trace.
This service is used as fast way to check if a proposed batch or batches of transactions are properly built and if they fit in a single execution trace. 
In other words, if the transactions of the batch or batches fit into a single proof.
The interface of this service is defined at the file \href[]{https://github.com/0xPolygonHermez/zkevm-prover/blob/main/src/grpc/proto/executor.proto}{executor.proto}.

The \textbf{StateDB service} provides an interface to access to database of the L2 state. 
This state is succinctly represented by the root of a Merkle tree and the service provides 
the corresponding data and their Merkle proofs.
This service is used by the Executor component and by the Prover component and it represents the single source of truth 
for the current state.
Among others, we can obtain account balances, nonce values, contract deployted bytecode, storage and so on.
The interface of this service is defined at the file \href[]{https://github.com/0xPolygonHermez/zkevm-prover/blob/main/src/grpc/proto/statedb.proto}{statedb.proto}. 

Finally, the \textbf{Aggregator service} offers the highest level of service among all the services provided by the zkEVM prover.
In more detail, this service can create a proof for a single batch or a proof for multiple batches if the fit in the execution trace.
In addition, this service is able to create an aggregated proof if a pair of  independent proofs of consecutive batches are provided.
When the Aggregator service is called to generate a proof, the service uses the Executor component to create the corresponding execution trace and from this trace the associated committed polynomials. 
Finally, with the pre-processed and committed polynomials the proof is generated.
When called with a pair of proofs of consecutive batches the service 
generates an aggregated proof.
The interface of this service is defined at the file \href[]{https://github.com/0xPolygonHermez/zkevm-prover/blob/main/src/grpc/proto/aggregator.proto}{aggregator.proto}. 

%TODO we can describe in more detail the messages (requests and responses)

\section{Appendix: Proof Composition with SNARKjs and PIL-STARK}
\input{../latex/circom-pil-power.tex}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{C12 PIL Description}

\subsection{From r1cs to \plonk}

Recall that the verifier procedure verifying a generated STARK is written in \texttt{circom}. This procedure is responsible for constructing a set of \texttt{r1cs} (Rank-1 Constraint System) constraints.  Additionally, it is crucial to incorporate the utilization of custom gates that were specified earlier. Observe that custom gates are not directly checked in \texttt{circom} and the \texttt{.r1cs} file only contains computational information about its inputs and outputs. Consequently, the verification process for custom gates must be performed explicitly within the generated \texttt{pil} code that describes the verifier's verification procedure. The \texttt{pil} code will be responsible for incorporating the necessary validations for the custom gates, ensuring that they function as intended within the overall verification process. 

In contrast to \plonk constraints, it is not straightforward to translate \texttt{r1cs} constraints into a fixed column execution trace along with a set of \texttt{pil} constraints. Therefore, it becomes necessary to perform a conversion from \texttt{r1cs} constraints to \plonk ones in order to proceed. In what follows, we describe this conversion. 

Recall that a ($m$-dimensional) Rank-1 Constraint System (\texttt{r1cs}) over a set of signals $s_1, \dots, s_n$ is composed of three matrices $A = (a_{i, j}), B = (b_{i, j}),  C = (c_{i, j})$ each beloving to the matrix space $\mathrm{M}(m, n, \FF)$ where \FF represents the underlying field. We say that $s = (s_1, \dots, s_n)$ satisfy the constraints if and only if 
\[
As \circ Bs = Cs.
\]
where $\circ$ denotes the Hadamard product (or component-wise multiplication). More explicitly, $s_1, \dots, s_n$ satisfies the constraints if and only if
\begin{align*}
\left( a_{1, 1} s_1 + \dots + a_{1, n} s_n \right) &\cdot \left( b_{1, 1} s_1 + \dots + b_{1, n} s_n \right) = c_{1, 1} s_1 + \dots + c_{1, n} s_n \\
&\dots  \\
\left( a_{m, 1} s_1 + \dots + a_{m, n} s_n \right) &\cdot \left( b_{m, 1} s_1 + \dots + b_{m, n} s_n \right) = c_{m, 1} s_1 + \dots + c_{m, n} s_n \\
\end{align*}


Based on the previous explanation, it can be noted that the constraint $(1 + s_2) \cdot s_3 = s_4$ is not allowed in the R1CS formulation. In order to address the inclusion of constants in constraints, a designated signal $s_1$ is introduced to always maintain a value of $1$.

Notice that we can perceive \texttt{r1cs} constraints as gates with an unbounded fan-in (meaning they can have any number of input wires) and an unbounded fan-out (meaning they can have any number of output wires). This flexibility allows \texttt{r1cs} to accommodate a wide range of computations. On the other hand, \plonk gates operate with a fixed fan-in of $2$ (meaning that each gate has exactly two input wires) and fan-out of $1$ (meaning that each gate has exactly one output wire). A single \plonk gate with inputs signals $a, b$ and output signal $c$ is determined by $5$ selectors, namely $q_R, q_L, q_M, q_O$ and $q_C$ and we say that the tuple $(a, b, c)$ satisfies the gate equation if and only if
\[
a_R \cdot a + q_L \cdot b + q_M \cdot a \cdot b + q_O \cdot c + q_C = 0.
\]

The transformation process from a Rank-1 Constraint System to a set of \plonk constraints involves mapping each individual \texttt{r1cs} constraint to one or more \plonk constraints. To ensure a correct, optimized and comprehensive conversion, the reduction process can be divided into distinct cases, each addressing a specific scenario. Within each case, specific rules and mappings will be defined to convert the \texttt{r1cs} constraints into the appropriate \plonk constraints. These rules take into consideration the unique characteristics of each case, such as the types of variables involved, the operations performed, and the desired properties in the resulting \plonk constraints. The strategy for all of them consists on reducing sums adding constraints and variables. More specifically, consider the following linear combination $a_1 + a_2 s_2 + \dots + a_n s_n$, which has $n-1$ non-constant therms. We can reduce this linear combination to contain $n-2$ non-constant therms adding another artificial signal, say $v_1$, and the constraint $a_{n-1} s_{n-1} + a_n s_n = v_1$ (with the corresponding \plonk selectors $q_L = - a_{n-1}, q_R = - a_n, q_M = 0, q_O = 1$ and $q_C = 0$). Replacing this into the linear combination we get a new reduced linear combination $a_1 + a_2 s_2 + \dots + v_1$. We can perform the same strategy recursively to further reduce the linear combination to one having only two therms $a_1 + v_{T_a}$, where $T_a$ denotes the total number of newly added variables. Let us discuss the complete strategy for each of the cases:

\begin{itemize}


\item In constraints where $A = 0$ and/or $B = 0$, we get a linear constraint of the form
\[
c_{1} + c_{2} s_2 + \dots + c_{n} s_n = 0.
\]

The idea to optimally attack this case is to reduce apply the reduction strategy in order to reduce the number of additions to $4$ ($3$ non-constant factors and one reserved for the constant). More specifically, the reduced constraint will look like:
\[
c_1 + c_2 s_2 + c_3 s_3 + v_T = 0,
\]
where $T$ denotes the total amount of newly introduced signals. At this point we can directly convert the upper constraint into \plonk by setting $q_R = c_2, q_L = c_3, q_M = 0, q_O = 1$ and $q_C = c_1$. 

% Normalization is only in charge of deleting signals with constant equal to 0. 

\item In constraints with $a_i = 0$ except when $i = 1$ (corresponding exactly to the constant wire), we get a linear constraint of the form
\[
a_1 \cdot (b_{1} + b_{2} s_2 + \dots + b_{n} s_n) = c_{1} + c_{2} s_2 + \dots + c_{n} s_n.
\]

In this case,  In this case, the strategy is to multiply the constant $a_1$ with the linear combination and combine both sides of the equation:
\begin{align*}
&a_1 \cdot (b_{1} + b_{2} s_2 + \dots + b_{n} s_n) = c_{1} + c_{2} s_2 + \dots + c_{n} s_n \Longleftrightarrow \\
&a_1 b_1 + a_1 b_2 s_2 + \dots + a_1 b_n s_n  = c_{1} + c_{2} s_2 + \dots + c_{n} s_n \Longleftrightarrow \\
&a_1 b_1 + a_1 b_2 s_2 + \dots + a_1 b_n s_n - c_1 - c_2 s_2 - \dots - c_n s_n = 0
\end{align*}

Now, we proceed as before and we apply the reduction strategy in order to reduce the number of additions to $4$ ($3$ non-constant factors and one reserved for the constant). More specifically, the reduced constraint will look like:
\[
a_1 b_1 + a_1 b_2 s_2 + a_1 b_3 s_3 + a_1 v_T = 0,
\]
where $T$ denotes the total amount of newly introduced signals. At this point we can directly convert the upper constraint into \plonk by setting $q_R = a_1 \cdot b_2, q_L = a_1 \cdot b_3, q_M = 0, ¨q_O = a_1$ and $q_C = a_1 b_1$. 

\item In constraints with $b_i = 0$ except when $i = 1$ (corresponding exactly to the constant wire), we get a linear constraint of the form
\[
(a_{1} + a_{2} s_2 + \dots + a_{n} s_n) \cdot b_1 = c_{1} + c_{2} s_2 + \dots + c_{n} s_n.
\]

These constraints are handled symmetrically to the previous method described.

\item In this case fit all the other scenarios, so it deals with general constraints 
\[
(a_{1} + a_{2} s_2 + \dots + a_{n} s_n) \cdot (b_{1} + b_{2} s_2 + \dots + b_{n} s_n) = c_{1} + c_{2} s_2 + \dots + c_{n} s_n
\]
excluding the cases where each $a_i$ or $b_i$ is zero or where only $a_1$ (or $b_1$) differs from zero. In this case, the solution is to reduce each of the linear combinations to a single coefficient and a constant, say: 
\begin{align*}
&(a_1 + a \cdot v_{T_a}) \cdot (b_1 + b \cdot v_{T_b}) = c_1 + c \cdot v_{T_c} \Longleftrightarrow \\
&(a_1 + b_1 - c_1) + a_1 \cdot b  v_{T_b} + b_1 \cdot a \cdot v_{T_a} + a \cdot b \cdot v_{T_a} \cdot v_{T_b} - c \cdot v_{T_c} = 0
\end{align*}

We can directly convert the previous constraint to \plonk by setting $q_L = a \cdot b_1, q_R = a_1 \cdot b, q_M = a \cdot b, q_C = a_1 + b_1 - c_1$ and $q_O = -c$.  

\end{itemize}



\subsection{C12 Plonk Gates Verification}

Once the \texttt{r1cs} has been transformed into a set of \plonk constraints, we can proceed to generate an execution trace along with a corresponding \texttt{pil} file for its verification. To ensure optimal verification of the \POSEIDON custom gates, it is preferable to use an execution trace with a width of 12 columns. Consequently, the state update of a single round of a Poseidon hash can be verified by examining the transition between two rows, without requiring additional rows. 

However, when verifying regular \plonk gates in \texttt{pil} using 12 columns, we encounter a wastage of 7 constant columns. This wastage occurs because we only require 5 columns to accommodate the constants of the constraint. To address this issue, we can utilize 2 sets of constraints in each row, enabling the verification of 2 \plonk constraints within a single row. It's important to note that \plonk gates have a fan-in of 2 and a fan-out of 1. Consequently, by utilizing only 2 sets of signals per row, we end up wasting 6 witness columns. To address this issue, we can employ a technique that involves reusing constraints by utilizing the connection arguments available in \texttt{pil}. This approach allows us to optimize the allocation of witness columns and minimize wastage. 

The idea is simple. At a single row of our execution trace we will have two sets of \plonk constraints, namely $Q = \{ q_L, q_R, q_O, q_M, q_C \}$ and $Q' = \{ q_L', q_R', q_O', q_M', q_C' \}$. The initial six witness columns $(\acode[0], \dots, \acode[5])$ will correspond to $Q$, while the remaining six witness columns $(\acode[6], \dots, \acode[11])$ will relate to $Q'$. More specifically, the following constraints will be necessary to be fulfilled: 

\begin{align*}
&\acode[0] \cdot q_L + \acode[1] \cdot q_R + \acode[2] \cdot q_O + \acode[0] \cdot  \acode[1] \cdot q_M + q_C = 0 \\
&\acode[3] \cdot q_L + \acode[4] \cdot q_R + \acode[5] \cdot q_O + \acode[3] \cdot  \acode[4] \cdot q_M + q_C = 0 \\
&\acode[6] \cdot q_L' + \acode[7] \cdot q_R' + \acode[9] \cdot q_O' + \acode[6] \cdot  \acode[7] \cdot q_M' + q_C' = 0 \\
&\acode[9] \cdot q_L' + \acode[10] \cdot q_R' + \acode[11] \cdot q_O' + \acode[9] \cdot  \acode[10] \cdot q_M' + q_C' = 0
\end{align*}

In \texttt{pil} code, this is translated to:

\begin{pil}
pol a01 = a[0]*a[1];
pol g012 = C[3]*a01 + C[0]*a[0] + C[1]*a[1] + C[2]*a[2] + C[4];
g012*GATE = 0;

pol a34 = a[3]*a[4];
pol g345 = C[3]*a34 + C[0]*a[3] + C[1]*a[4] + C[2]*a[5] + C[4];
g345*GATE = 0;

pol a67 = a[6]*a[7];
pol g678 = C[9]*a67 + C[6]*a[6] + C[7]*a[7] + C[8]*a[8] + C[10];
g678*GATE = 0;

pol a910 = a[9]*a[10];
pol g91011 = C[9]*a910 + C[6]*a[9] + C[7]*a[10] + C[8]*a[11] + C[10];
g91011*GATE = 0;
\end{pil}

However, to ensure soundness and achieve the desired order of witness signals, we need to utilize connection arguments. These gates enable us to verify that the signals appearing in a specific \plonk constraint are accurate.

If we fail to assert this, the prover could incorrectly claim that:
\[
s_1 + s_2 = s_3 \quad \text{and} \quad s_4 - s_5 = s_6
\]
while the actual \plonk constraint is:
\[
s_4 + s_5 = s_6 \quad \text{and} \quad s_1 - s_2 = s_3
\]

Without ensuring the correct ordering of the witness signals, the prover could manipulate the equations and present an inaccurate representation of the \plonk constraint. By asserting the correct connection between the witness signals and the constraints, we prevent such incorrect claims and guarantee the accurate representation of the desired \plonk constraint. To establish this, we employ the following \texttt{pil} constraint:

\begin{pil}
{a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8], a[9], a[10], a[11]} connect
    {S[0], S[1], S[2], S[3], S[4], S[5], S[6], S[7], S[8], S[9], S[10], S[11]};
\end{pil}

where the $\texttt{S}$ polynomials keeps track of the exact permutation that corresponds to the position of the constraints we have deliberately placed along the execution trace.

\subsection{Poseidon Round Verification}

Whenever \POSEIDON is $1$, the PIL file will check that the vector 
\[
(\acode[0] \nextStep, \acode[1] \nextStep, \acode[2] \nextStep, \acode[3] \nextStep, \acode[4] \nextStep, \acode[5] \nextStep, \acode[6] \nextStep, \acode[7] \nextStep, \acode[8] \nextStep, \acode[9] \nextStep, \acode[10] \nextStep, \acode[11] \nextStep)
\]

is obtained applying the \texttt{POSEIDON} permutation to the vector
\[
(\acode[0], \acode[1], \acode[2], \acode[3], \acode[4], \acode[5], \acode[6], \acode[7], \acode[8], \acode[9], \acode[10], \acode[11]).
\]

Recall that the \texttt{POSEIDON} permutation has two modes: the partial permutation and the full one. The partial permutation is characterized by only-applying the S-Box layer to one element of the state (the first one, for example). However, a full round applies the S-Box to the whole set of $12$ elements which forms the state. Hence, we need a polynomial called \PARTIAL which will distinguish if the current round is a partial round or a full round. More concretely, \PARTIAL constant polynomial will be $1$ if and only if the current round is a partial round and $0$ otherwise. 

First of all, we will compute, if necessary, the $7$-th power of each element of the state in order to compute the S-Box layer. Moreover, we will add, at the beggining of the computation, the corresponding constant specified by the \texttt{Poseidon} permutation, which are stored in the polynomials \texttt{C[i]}. We will use the following code (written using \texttt{ejs}):

\begin{pil}
pol a<%- i %>_1 = a[<%- i %>] +  C[<%- i %>];
pol a<%- i %>_2 = a<%- i %>_1 * a<%- i %>_1;
pol a<%- i %>_4 = a<%- i %>_2 * a<%- i %>_2;
pol a<%- i %>_6 = a<%- i %>_4 * a<%- i %>_2;
pol a<%- i %>_7 = a<%- i %>_6 * a<%- i %>_1;
<%      if (i==0) { -%>
pol a<%- i %>_R = a<%- i %>_7;
<%      } else { -%>
pol a<%- i %>_R = PARTIAL * (a<%- i %>_1 - a<%- i %>_7) + a<%- i %>_7; 
<%      } -%>
<% } -%>
\end{pil}

We will carry $\texttt{ai\_R}$ to the next phase of the permutation to multiply it by the corresponding MDS matrix. Observe that we always exponentiate when $i=0$ (that is, the first element of the state). However, the other elements are only exponentiated if and only if \PARTIAL is $1$. The last part of the validation is trivial to validate:

\begin{pil}
POSEIDON12 * (a[0]'  - (25*a0_R + 15*a1_R + 41*a2_R + 16*a3_R +  2*a4_R + 28*a5_R + 13*a6_R + 13*a7_R + 39*a8_R + 18*a9_R + 34*a10_R + 20*a11_R)) = 0;
POSEIDON12 * (a[1]'  - (20*a0_R + 17*a1_R + 15*a2_R + 41*a3_R + 16*a4_R +  2*a5_R + 28*a6_R + 13*a7_R + 13*a8_R + 39*a9_R + 18*a10_R + 34*a11_R)) = 0;
POSEIDON12 * (a[2]'  - (34*a0_R + 20*a1_R + 17*a2_R + 15*a3_R + 41*a4_R + 16*a5_R +  2*a6_R + 28*a7_R + 13*a8_R + 13*a9_R + 39*a10_R + 18*a11_R)) = 0;
POSEIDON12 * (a[3]'  - (18*a0_R + 34*a1_R + 20*a2_R + 17*a3_R + 15*a4_R + 41*a5_R + 16*a6_R +  2*a7_R + 28*a8_R + 13*a9_R + 13*a10_R + 39*a11_R)) = 0;
POSEIDON12 * (a[4]'  - (39*a0_R + 18*a1_R + 34*a2_R + 20*a3_R + 17*a4_R + 15*a5_R + 41*a6_R + 16*a7_R +  2*a8_R + 28*a9_R + 13*a10_R + 13*a11_R)) = 0;
POSEIDON12 * (a[5]'  - (13*a0_R + 39*a1_R + 18*a2_R + 34*a3_R + 20*a4_R + 17*a5_R + 15*a6_R + 41*a7_R + 16*a8_R +  2*a9_R + 28*a10_R + 13*a11_R)) = 0;
POSEIDON12 * (a[6]'  - (13*a0_R + 13*a1_R + 39*a2_R + 18*a3_R + 34*a4_R + 20*a5_R + 17*a6_R + 15*a7_R + 41*a8_R + 16*a9_R +  2*a10_R + 28*a11_R)) = 0;
POSEIDON12 * (a[7]'  - (28*a0_R + 13*a1_R + 13*a2_R + 39*a3_R + 18*a4_R + 34*a5_R + 20*a6_R + 17*a7_R + 15*a8_R + 41*a9_R + 16*a10_R +  2*a11_R)) = 0;
POSEIDON12 * (a[8]'  - ( 2*a0_R + 28*a1_R + 13*a2_R + 13*a3_R + 39*a4_R + 18*a5_R + 34*a6_R + 20*a7_R + 17*a8_R + 15*a9_R + 41*a10_R + 16*a11_R)) = 0;
POSEIDON12 * (a[9]'  - (16*a0_R +  2*a1_R + 28*a2_R + 13*a3_R + 13*a4_R + 39*a5_R + 18*a6_R + 34*a7_R + 20*a8_R + 17*a9_R + 15*a10_R + 41*a11_R)) = 0;
POSEIDON12 * (a[10]' - (41*a0_R + 16*a1_R +  2*a2_R + 28*a3_R + 13*a4_R + 13*a5_R + 39*a6_R + 18*a7_R + 34*a8_R + 20*a9_R + 17*a10_R + 15*a11_R)) = 0;
POSEIDON12 * (a[11]' - (15*a0_R + 41*a1_R + 16*a2_R +  2*a3_R + 28*a4_R + 13*a5_R + 13*a6_R + 39*a7_R + 18*a8_R + 34*a9_R + 20*a10_R + 17*a11_R)) = 0;
\end{pil}

Numbers above are determined by the used MDS matrix of the permutation. More precisely, we are checking the matrix product shown below:
\setcounter{MaxMatrixCols}{20}
\[
\begin{pmatrix}
25 &15 &41 &16 &2 &28 &13 &13 &39 &18 &31 &20 \\
20 &17 &15 &41 &16 &2 &28 &13 &13 &39 &18 &34 \\
34 &20 &17 &15 &41 &16 &2 &28 &13 &13 &39 &18 \\
18 &34 &20 &17 &15 &41 &16 &2 &28 &13 &13 &39 \\
39 &18 &34 &20 &17 &15 &41 &16 &2 &28 &13 &13 \\
13 &39 &18 &34 &20 &17 &15 &41 &16 &2 &28 &13 \\
13 &13 &39 &18 &34 &20 &17 &15 &41 &16 &2 &28 \\
28 &13 &13 &39 &18 &34 &20 &17 &15 &41 &16 &2 \\
2  &28 &13 &13 &39 &18 &34 &20 &17 &15 &41 &16 \\
16 &2  &28 &13 &13 &39 &18 &34 &20 &17 &15 &41 \\
41 &16 &2  &28 &13 &13 &39 &18 &34 &20 &17 &15 \\
15 &41 &16 &2 &28 &13 &13 &39 &18 &34 &20 &17 
\end{pmatrix}
\cdot 
\begin{pmatrix}
\texttt{a0\_R} \\
\texttt{a1\_R} \\
\texttt{a2\_R} \\
\texttt{a3\_R} \\
\texttt{a4\_R} \\
\texttt{a5\_R} \\
\texttt{a6\_R} \\
\texttt{a7\_R} \\
\texttt{a8\_R} \\
\texttt{a9\_R} \\
\texttt{a10\_R} \\
\texttt{a11\_R}
\end{pmatrix}
= 
\begin{pmatrix}
\texttt{a[0]} \nextStep \\
\texttt{a[1]} \nextStep \\
\texttt{a[2]} \nextStep \\
\texttt{a[3]} \nextStep \\
\texttt{a[4]} \nextStep \\
\texttt{a[5]} \nextStep \\
\texttt{a[6]} \nextStep \\
\texttt{a[7]} \nextStep \\
\texttt{a[8]} \nextStep \\
\texttt{a[9]} \nextStep \\
\texttt{a[10]} \nextStep \\
\texttt{a[11} \nextStep
\end{pmatrix}
\]



\subsection{Extended Field Operations Verification} \label{sec:c12-cmuladd}

Whenever \CMULADD is $1$, the PIL file will check that the following elements of $\FF_{p^3}$
\begin{align*}
a &= \acode[0] + \acode[1] \cdot X + \acode[2] \cdot X^2 \\
b &= \acode[3] + \acode[4] \cdot X + \acode[5] \cdot X^2 \\
c &= \acode[6] + \acode[7] \cdot X + \acode[8] \cdot X^2 \\
\texttt{output} &= \acode[9] + \acode[10] \cdot X + \acode[11] \cdot X^2
\end{align*}
satisfy the relationship
\[
a \cdot b + c = \texttt{output}
\]
using the field operations inherited from 
\[
\FF_{p^3} \cong \FF_p[X]/(X^3 - X - 1).
\]

Given two elements $a_0 + a_1 X + a_2 X^2, b_0 + b_1 X + b_2 X^2 \in \FF_{p^3}$, we can express its product by computing the product of the polynomials, using Euclidean division and taking equivalence classes in $\FF_{p^3}$. It is not difficult to see, then, that we can express its product as
\begin{align*}
&(a_0 \cdot b_0 + a_1 \cdot b_2 + a_2 \cdot b_1) + \\
&(a_0 \cdot b_1 + a_1 \cdot b_0 + a_1 \cdot b_2 + a_2 \cdot b_1 + a_2 \cdot b_2) \cdot X + \\ 
&(a_0 \cdot b_2 + a_2 \cdot b_2 + a_2 \cdot b_0 + a_1 \cdot b_1) \cdot X^2
\end{align*}

Hence, the PIL code below states the correctness of the output elements $\acode[9], \acode[10], \acode[11]$ using polynomial relationships of degree less or equal than $2$ using the operation defined above:

\begin{pil}
pol cA = (a[0] + a[1]) * (b[0] + b[1]); 
pol cB = (a[0] + a[2]) * (b0 + b[2]);
pol cC = (a[1] + a[2]) * (b[1] + b[2]);
pol cD = a[0] * b[0];
pol cE = a[1] * b[1];
pol cF = a[2] * b[2];

CMULADD * (a[9] - (cC + cD - cE - cF) - c0) = 0;
CMULADD * (a[10] - (cA + cC - 2*cE - cD) - c1) = 0;
CMULADD * (a[11] - (cB - cD + cE) - c2) = 0;
\end{pil}

The previous piece of code works since:
\begin{align*}
&\texttt{cA} = (\texttt{a0} + \texttt{a1}) \cdot (\texttt{b0} + \texttt{b1}) = \texttt{a0} \cdot \texttt{b0} + \texttt{a0} \cdot \texttt{b1} + \texttt{a1} \cdot \texttt{b0} + \texttt{a1} \cdot \texttt{b1} \\
&\texttt{cB} = (\texttt{a0} + \texttt{a2}) \cdot (\texttt{b0} + \texttt{b2}) = \texttt{a0} \cdot \texttt{b0} + \texttt{a0} \cdot \texttt{b2} + \texttt{a2} \cdot \texttt{b0} + \texttt{a2} \cdot \texttt{b2} \\
&\texttt{cC} = (\texttt{a1} + \texttt{a2}) \cdot (\texttt{b1} + \texttt{b2}) = \texttt{a1} \cdot \texttt{b1} + \texttt{a1} \cdot \texttt{b2} + \texttt{a2} \cdot \texttt{b1} + \texttt{a2} \cdot \texttt{b2}
\end{align*}
Hence
\begin{align*}
&\acode[9] = \texttt{cC} + \texttt{cD} - \texttt{cE} - \texttt{cF} + \texttt{c0} = \texttt{a0} \cdot \texttt{b0} + \texttt{a1} \cdot \texttt{b2} + \texttt{a2} \cdot \texttt{b1} \\
&\acode[10] = \texttt{cA} + \texttt{cC} - 2 \texttt{cE} - \texttt{cD} + \texttt{c1} = \texttt{a0} \cdot \texttt{b1} + \texttt{a1} \cdot \texttt{b0} + \texttt{a1} \cdot \texttt{b2} + \texttt{a2} \cdot \texttt{b1} + \texttt{a2} \cdot \texttt{b2} \\
&\acode[11] = \texttt{cB} - \texttt{cD} + \texttt{cE} + \texttt{c2} = \texttt{a0} \cdot \texttt{b2} + \texttt{a2} \cdot \texttt{b2} + \texttt{a2} \cdot \texttt{b0} + \texttt{a1} \cdot \texttt{b1}
\end{align*}







\subsection{Fast Fourier Transform Verification}

Whenever \FFT is $1$, the PIL file will check the correct computation of a Fast Fourier Transform using the Coley-Tucker's butterfly method. Depending on the custom gate being validated, the input of the FFT can be $2$ elements or $4$ elements. Take into account that we apply the FFT to extended field elements, consisting on $3$ field elements. Depending on the number of input elements, the constants for the computation \CONST are adjusted in order to mimic the butterfly's formulas. Moreover, a parameter called \texttt{scale} is introduced in order to be able to perform inverse Fast Fourier Transforms. 

However, we should be able to perform much bigger FFTs, so the custom gate is programmed in order to optimize the computation using the necessary number of four-size FFT and, if necessary, complete them with the complementary number of two-sized FFT (observe that only $1$ of them may be necessary). Hence, we would also need a mechanism to chain them correspondingly following the butterfly diagram. 




\subsubsection{How to Chain FFTs}

First of all suppose that we want to perform the Fast Fourier Transform of $n$ elements in the base field $\FF_p$ where $n$ is a power of two having its exponent $\log_2(n)$ even. We are choosing a even exponent for the power of two since we will discuss first the case where no $2$-sized FFT are needed. The idea of the FFT is to reuse computations in order to reduce complexity. This can be seen easily using the butterfly diagram. Below, we will show the butterfly diagram for the case $n = 16$. We can reduce the total computation of the FFT for $16$ elements to a total amount of $8$ FFT of $4$ elements. However, chained FFT needs to be readapted and well-connected in order to express the correct computation. Observe that, before the first step, we should reverse the bits of the polynomial coefficients' indices in order to correctly order them all.


\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{\recursiondir/figures/16-elements-fft}
\caption{Butterfly diagram for 16 elements' Fast Fourier Transform.}
\label{fig:c12a-proof}
\end{figure}

Let us denote each of the $r + 1$ states of the $n$-sized Fast Fourier Transform as $(f_0^k, \dots, f_{n-1}^k)$, where $r = \log_2(n)$. The initial state $(f_0^0, \dots, f_{n-1}^0)$, representing the input, is labeled as step $0$, while the final state $(f_0^r, \dots, f_{n-1}^r)$, representing the output, is labeled as step $r$. Alternatively and aiming to clear pictures, we will also use $f_0, \dots, f_{n-1}$ for the input and $y_0, \dots, y_{n-1}$ for the output of the whole FFT. First of all, recall that, when fixing a certain step $k \in \{0, \dots, r - 1\}$, the next state $(f_0^{k+1} \dots, f_{n-1}^{k+1})$ of the butterfly structure can be directly computed from the previous one $(f_0^{k}, \dots, f_{n-1}^{k})$ as follows:
\[
f_j^{k+1} = f_j^{k} + f_{j + 2^k}^{k} \cdot \omega_{2^{k+1}}^j, \quad j \in \{0, \dots, n-1\}
\]
being $\omega_{2^{k+1}} \in \FF_p^*$ a primitive $2^{k+1}$-root of unity. However, we can optimize the computation by exploiting the fact that:
\[
- \omega_{2^{k+1}}^j = \omega_{2^{k+1}}^{j + 2^k} \quad \text{ for } j \in \{0, \dots, 2^k - 1\}.
\]
By utilizing this property, we can avoid calculating half of the powers of $\omega_{2^{k+1}}$. Hence, we get the following optimized butterfly diagram:

\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{\recursiondir/figures/optimized-16-elements-fft}
\caption{Optimized butterfly diagram for 16 elements' Fast Fourier Transform.}
\label{fig:c12a-proof}
\end{figure}

%TODO: Write the formulas for the optimized version

In the picture above it is important to observe some pattern. The components $y_0, y_4, y_8$ and $y_{12}$ of the output of the whole FFT of $16$ elements actually depend on the same intermediate $4$ coefficients $f_0^2, f_4^2, f_8^2, f_{12}^2$ appearing in the step $2$ of the FFT. In the picture below we show in red the wires that are connected to the components $y_0, y_4, y_8$ and $y_{12}$ and we can easily see that the previous statement is true. But this is not the only coincidence. 

\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{\recursiondir/figures/16-elements-fft-connection}
\caption{Optimized butterfly diagram for 16 elements' Fast Fourier Transform.}
\label{fig:c12a-proof}
\end{figure}

Observe that, in fact, the subgraph formed by the red wires is actually another FFT of $4$ elements, with the output being $f_0^2, f_4^2, f_8^2$ and $f_{12}^2$. However, observe that, in this step, the used roots have to be modified accordingly. We depict the extracted subgraph in the figure below, in order to be able to see that in a clear way. 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{\recursiondir/figures/fft-subgraph}
\caption{Optimized butterfly diagram for 16 elements' Fast Fourier Transform.}
\label{fig:c12a-proof}
\end{figure}

\newcommand{\bigslant}[2]{{\raisebox{.2em}{$#1$}\left/\raisebox{-.2em}{$#2$}\right.}}

To provide a more specific explaination, let's focus on determining the elements $f_j^k$ that belong to the same $4$-sized FFT at the step $k$, where $k$ is an even integer and $n$ is a power of $4$ (so that no $2$-sized FFT are needed at the end). Mathematically, we aim to compute a set of representatives for the classes of equivalence $\bigslant{\{f^k_j\}_{j=0}^{n-1}}{\thicksim^k}$, where $\thicksim^k$ denotes the equivalence relation $f_{j}^k \thicksim^k f_{j'}^k$ if they belong to the same $4$-sized FFT. Notice that if $f_j^{k+2}$ belongs to a equivalence class $f_j^k \in \bigslant{\{f^k_j\}_{j=0}^{n-1}}{\thicksim^k}$, where $j$ is the smallest index among all the elements in the class, then all the elements of its class can be expressed as:
\[
\left( f_{j}^{k}, f_{j + 2^{k} }^{k}, f_{j + 2 \cdot 2^{k}}^{k}, f_{j + 3 \cdot 2^{k}} ^{k} \right) 
\]


The equivalence relation in the set $\{f_0^k, \dots, f_{n-1}^k \}$ naturally induces an equivalence relation in the set of indices $\{0, \dots, n-1\}$. Therefore, we can interchangeably consider both perspectives. The first representative of each class is given by $f^k_{S_j^k}$, where $S_j^k$ is given by the sequence $\left( S_j^k \right)_j$ for $j \in \{0,\hdots,n/4-1\}$:
\begin{align*}
&S_j^k=S_{j-1}^k + 1                          &\text{if }S_{j-1}^k + 1 \not \equiv 0 \pmod{2^k} \\
&S_j^k=S_{j-1}^k + 3\cdot 2^k + 1           &\text{if }S_{j-1}^k + 1 \equiv 0 \pmod{2^k}
\end{align*}

Let us examine why the previous formula works more carefully. The following example provides a list of the corresponding indices for all the elements in each equivalence class, arranged in the natural order and computed by examining the butterfly diagram. This example considers the case of $n=64$ and $k=2$:

\begin{align*}
& (0,4,8,12), (1,5,9,13), (2,6,10,14), (3,7,1,15), \\
& (16, 20,21,28), (17,21,25,29), (18,22,26,30), (19,2,27,31), \\
& (32,36,40,44), (33,37,41,45), (34, 38, 42, 46), (35, 39, 43, 47), \\
& (48, 52, 56, 60), (49, 53, 57, 61), (50, 54, 58, 62), (51, 55, 59, 63)
\end{align*}
being each of the first representatives $S_j^k$ the following sequence of indexes
\[
0, 1, 2, 3, 16, 17, 18, 19, 32, 33, 34, 35, 48, 49, 50 \text{ and } 51.
\]


We notice that the sequence starts with $0, 1, 2, 3$, but there is an abrupt jump to $16$. Tis jump occurs because the index $4$ is already present in the class of $0$. Hence, we need to jump to the next free slot. We can compute this free slot from the last computed index, in this case $15$ (which is computed from $S_3^2$ as $S_3^2 + 3 \cdot 2^2$), and add one:
\[
S_4^2 = S_3^2 + 3 \cdot 2^2 + 1.
\]

To correctly determine all the values of $S_j^k$, it is essential to identify when these jumps occur. To do so, observe that we only need to check whether the consecutive index $S_{j-1}^k + 1$ is a multiple of $2^k$ or not. If $S_{j-1}^k + 1$ is a multiple, it indicates that $S_{j-1}^k + 1$ is already present in the computed classes.  In such cases, we need to compute the next index $S_j^k$ by adding $1$ to the last element of the current class, which is $S_{j-1}^k + 3 \cdot 2^k$. If not, we only need to add one to the current index, since the next one is not present yet. 


For instance, in the given example, we can see that $15$ satisfies the relation
\[
2^k = 2^2 = 4 \mid 4 = 3 + 1 = S_{3} + 1
\]
while
\begin{align*}
& 4 \nmid 1 = 0 + 1 = S_{0} + 1 \\
& 4 \nmid 2 = 1 + 1 = S_{1} + 1 \\
& 4 \nmid 3 = 2 + 1 = S_{2} + 1.
\end{align*}

Once we have defined the set of classes for each step $k$ of the FFT, we can compute the next state corresponding to step $k + 2$ by independently computing the $4$-sized FFT of each class. It's important to note that, since we are in the step $k$ of the $n$-sized FFT, we will need the values of $\omega_{2^k}$ and $\omega_{2^{k+1}}$. However, recall that we have the following fact:
\[
\omega_{2^{k+1}}^2 = \omega_{2^{k}}.
\]
For $k \in \{ 0, 2, \dots, \log_2(n) - 2 \}$, we can establish the following relationships:
\begin{align*}
& f_{S_j^k}^{k+2} = f_{S_j^k}^k + \omega_{2^{k+2}}^{\mathfrak{s}_j^k} \cdot f_{j + 2 \cdot 2^k}^k + \omega_{2^{k+2}}^{2{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 2^k}^k + \omega_{2^{k+2}}^{3{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 3 \cdot 2^k}^k \\
& f_{S_j^k + 2^{k+2}}^{k+2} = f_{S_j^k}^k + \omega_2 \cdot \omega_{2^{k+2}}^{\mathfrak{s}_j^k} \cdot f_{S_j^k + 2 \cdot 2^k}^k - \omega_{2^{k+2}}^{2{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 2^k}^k  - \omega_2 \cdot \omega_{2^{k+2}}^{3{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 3 \cdot 2^k}^k \\
& f_{S_j^k + 2 \cdot 2^{k+2}}^{k+2} = f_{S_j^k}^k - \omega_{2^{k+2}}^{\mathfrak{s}_j^k} \cdot f_{S_j^k + 2 \cdot 2^k}^k + \omega_{2^{k+2}}^{2{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 2^k}^k - \omega_{2^{k+2}}^{3{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 3 \cdot 2^k}^k \\
& f_{S_j^k + 2^{k+2}}^{k+2} = f_{S_j^k}^k - \omega_2 \cdot \omega_{2^{k+2}}^{\mathfrak{s}_j^k} \cdot f_{S_j^k + 2 \cdot 2^k}^k - \omega_{2^{k+2}}^{2{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 2^k}^k + \omega_2 \cdot \omega_{2^{k+2}}^{3{\mathfrak{s}_j^k}} \cdot f_{S_j^k + 3 \cdot 2^k}^k \\
\end{align*}
where 
\[
\mathfrak{s}_j^k = S_j^k \pmod{2^{k}}.
\]
The aforementioned equalities are derived from the fact that
\[
\omega^{j}_{2^{k+1}} \cdot \omega_2 = \omega^j_{2^{k+1}} \cdot \omega_{2^{k+1}}^{2k} =  \omega_{2^{k+1}}^{j + 2^k}
\]
which represents the two powers of $\omega_{2^{k+1}}$ associated with the same $4$-sized FFT class.

Now, let us move to the case where $n$ is an odd power of two (meaning that $\log_2(n)$ is odd). In the previous case, where $\log_2(n)$ is even, we only need to employ $4$-sized FFT chains. However, in this case, we must incorporate a final $2$-sized FFT step at the end of the whole process. At the last step, we will need to execute $n/2$ 2-sized FFTs, connecting $f_j^{r-1}$ and $f_{j+n/2}^{r-1}$ for $j \in \{ 0, \ldots, \frac{n}{2}-1 \}$. Thus, the formulas for the final step, applicable to $j \in \{ 0, \ldots, \frac{n}{2}-1 \}$, are as follows:
\begin{align*}
&f_j^{r} = f_j^{r-1} + f_{j+n/2}^{r-1} \cdot \omega_{n}^j \\
&f_{j+n/2}^{r} = f_{j+n/2}^{r-1} - f_{j}^{r-1} \cdot \omega_{n}^j
\end{align*}

Here, we have utilized the fact that $\omega_{n}^j = -\omega_{n}^{j+n/2}$.







\subsection{Polynomial Evaluation Verification}

Whenever \EVPOL is $1$, the PIL file will check that the value 
\[
\acode[6] \nextStep + \acode[7] \nextStep \cdot X + \acode[8] \nextStep \cdot X^2 \in \FF_{p^3}
\]
is obtained by evaluating the polynomial
\[
p(Z) = \texttt{d0} \cdot Z^4 + \texttt{d1} \cdot Z^3 + \texttt{d2} \cdot Z^2 + \texttt{d3} \cdot Z + \texttt{d4}
\]
at a given point 
\[
z = \acode[3] \nextStep + \acode[4] \nextStep \cdot X + \acode[5] \nextStep \cdot X^2
\]
being \texttt{d0}, \texttt{d1}, \texttt{d2}, \texttt{d3} and \texttt{d4} $\in \FF_{p^3}$ defined as:
\begin{align*}
&\texttt{d0} = \acode[0] \nextStep + \acode[1] \nextStep \cdot X + \acode[2] \nextStep \cdot X^2 \\
&\texttt{d1} = \acode[9] + \acode[10] \cdot X + \acode[11] \cdot X^2 \\
&\texttt{d2} = \acode[6] + \acode[7] \cdot X + \acode[8] \cdot X^2   \\
&\texttt{d3} = \acode[3] + \acode[4] \cdot X + \acode[5] \cdot X^2   \\
&\texttt{d4} = \acode[0] + \acode[1] \cdot X + \acode[2] \cdot X^2
\end{align*}

This evaluation will be done using Horner's rule, so that $p(Z)$ will be rewritten as
\[
p(Z) = (((\texttt{d0} \cdot Z + \texttt{d1}) \cdot Z + \texttt{d2}) \cdot Z + \texttt{d3}) \cdot Z + \texttt{d4}.
\]
Also observe that all the operations performed at the evaluation are operations in $\FF_{p^3}$, so we will perform the operations in PIL using the following \texttt{CMulAdd} function, written in \texttt{ejs}. Observe that the logic is exactly the same than the one presented in Section \ref{sec:c12-cmuladd}.

Note that the order selected to input the parameters of the \EVPOL gate is totally arbitrary and we should only have to take care that is \textbf{completely} aligned with the Circom's custom gate. Moreover, observe that, since there are more than $12$ parameters, they all not fit in a row of the execution trace, forcing us to use $2$ of them to include them. This is not even a problem since the validation only occupies $3$ columns. 

\begin{js}
<% function CMulAdd(s, a0, a1, a2, b0, b1, b2, c0, c1, c2) {
    const code = [];
    code.push(`pol A${s} = (${a0} + ${a1})  * (${b0} + ${b1});`);
    code.push(`pol B${s} = (${a0} + ${a2})  * (${b0} + ${b2});`);
    code.push(`pol C${s} = (${a1} + ${a2})  * (${b1} + ${b2});`);
    code.push(`pol D${s} = ${a0} * ${b0};`);
    code.push(`pol E${s} = ${a1} * ${b1};`);
    code.push(`pol F${s} = ${a2} * ${b2};`);
    code.push(`pol acc${s}_0 = C${s}+ D${s} - E${s} - F${s} + ${c0};`);
    code.push(`pol acc${s}_1 = A${s}+ C${s}- 2*E${s} - D${s} + ${c1};`);
    code.push(`pol acc${s}_2 = B${s}- D${s} + E${s} + ${c2};`);
    code.push(`\n`);
    return code.join("\n");
} -%>
\end{js}

Now, we can compute $p(z) \in \FF_{p^3}$ using the previous function, accumulating the results by using the Horner's rule:

\begin{js}
// acc = d0 * x + d1 
<%- CMulAdd(
        "1", 
        "a[0]'", 
        "a[1]'", 
        "a[2]'", 
        "a[3]'", 
        "a[4]'", 
        "a[5]'", 
        "a[9]", 
        "a[10]", 
        "a[11]"
    ) 
-%>
\end{js}

\begin{js}
// acc2 = acc * x + d2 
<%- CMulAdd(
        "2",
        "acc1_0", 
        "acc1_1", 
        "acc1_2", 
        "a[3]'", 
        "a[4]'", 
        "a[5]'", 
        "a[6]'", 
        "a[7]", 
        "a[8]"
    ) 
-%>
\end{js}

\begin{js}
// acc3 = acc2 * x + d3 
<%- CMulAdd(
        "3", 
        "acc2_0", 
        "acc2_1", 
        "acc2_2", 
        "a[3]'", 
        "a[4]'", 
        "a[5]'", 
        "a[3]", 
        "a[4]", 
        "a[5]"
    ) 
-%>
\end{js}

\begin{js}
// acc4 = acc3 * x + d4 
<%- CMulAdd(
        "4", 
        "acc3_0", 
        "acc3_1", 
        "acc3_2", 
        "a[3]'", 
        "a[4]'", 
        "a[5]'", 
        "a[0]", 
        "a[1]", 
        "a[2]"
    ) 
-%>
\end{js}

Now, the only what remains is to check that the last values obtained $\texttt{acc3\_0}, \texttt{acc3\_1}, \texttt{acc3\_2}$ are equal to the supposed committed evaluation of $p$ at $z$, $\acode[6] \nextStep, \acode[7] \nextStep$ and $\acode[8] \nextStep$ respectively:

\begin{pil}
EVPOL4 * (a[6]' - acc4_0) = 0;
EVPOL4 * (a[7]' - acc4_1) = 0;
EVPOL4 * (a[8]' - acc4_2) = 0;
\end{pil}




%\subsection{Depth-One Composition ??}
%
%In order to achieve a composed proof system $\F = \O \circ \I$, we must take the verification procedure of $\I$ and feeding it through the proof machinery of $\O$. That is, a typical workflow would represent the verifier in $\I$ as an arithmetic circuit $C$ and then apply the prover in $\O$ to establish knowledge of a proof $\pi_{\I}$ such that $C(\pi_{\I}) = 1$. A high-level description can be seen in Figure \ref{fig:recursive-depth1}.
%
%There are two natural ways to obtain proof system composition by means of PIL and Circom:
%\begin{enumerate}
% \item[(a)] Use PIL to represent the verifier in $\I$ as a state machine $M$ such that $M(\pi_{\I}) = 1$ if and only if $\pi_{\I}$ is valid, and then use $\texttt{pil-stark}$ to generate a proof for $M$.
% \item[(b)] Use \texttt{pil2circom} to represent the verifier in $\I$ as an arithmetic circuit $C$ written in Circom such that $C(\pi_{\I}) = 1$ if and only if $\pi_{\I}$ is valid, then use $\texttt{r1cs2plonk}$ to transform the R1CS intermediate representation into PIL, and then use $\texttt{pil-stark}$ to generate a proof for the state machine of the last PIL. This process is depicted in Figure \ref{fig:recursive-depth1}.
%\end{enumerate}
%
%\begin{figure}[H]
%\centering
%\includegraphics[width=\textwidth]{\recursiondir/figures/recursive-diagram-depth1}
%\caption{Depiction of a depth-one composition with focus on the workflow followed by the Prover to obtain the outer proof $\pi_{\O}$.}
%\label{fig:recursive-depth1}
%\end{figure}
%
%The Circom implementation of the inner STARK verification procedure can be found in \cite{HermezSTARKVerCircom2022}.